{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u-qFVy2lBaqu"
   },
   "source": [
    "# News Topic modeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mNfMRWDbFXDJ"
   },
   "source": [
    "## I- Modules import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "home_dir = os.path.expanduser(\"~\")\n",
    "home_dir\n",
    "\n",
    "# Construct the full paths to the JAR files\n",
    "os.environ['SPARK_HOME']=os.path.join(home_dir, 'spark-3.2.0')\n",
    "rapids_jar_path = os.path.join(home_dir, 'rapids-4-spark_2.12-21.12.0.jar')\n",
    "cudf_jar_path = os.path.join(home_dir, 'cudf-21.12.2-cuda11.jar')\n",
    "os.environ['JAVA_HOME'] = \"/home/saturne.ayidegnon/.asdf/installs/java/openjdk-11\"\n",
    "os.environ['PYSPARK_SUBMIT_ARGS']=f\"--jars {rapids_jar_path},{cudf_jar_path} --master local[*] pyspark-shell\"\n",
    "os.environ['PYSPARK_SUBMIT_ARGS']\n",
    "\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyspark==3.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "3QYNQcr-FQLs"
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.feature import  IDF, HashingTF,CountVectorizer\n",
    "from pyspark.ml import  Pipeline\n",
    "from math import ceil,log2\n",
    "from pyspark.ml.classification import LogisticRegression,NaiveBayes,LogisticRegressionModel\n",
    "from pyspark.sql.functions import col,explode,split\n",
    "\n",
    "import numpy as np\n",
    "from pyspark.ml.clustering import LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nj7Qu-EvK9_Y"
   },
   "source": [
    "## II- Spark context and session creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "id": "aFZWn6-QBByS",
    "outputId": "cc18968f-50af-4a70-9d51-126be08a96c8",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/08 01:45:32 WARN SparkContext: The path /home/saturne.ayidegnon/rapids-4-spark_2.12-21.12.0.jar has been added already. Overwriting of added paths is not supported in the current version.\n",
      "24/06/08 01:45:32 WARN SparkContext: The path /home/saturne.ayidegnon/cudf-21.12.2-cuda11.jar has been added already. Overwriting of added paths is not supported in the current version.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://node13.cm.cluster:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>SparkRAPIDS</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fffe80136d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = (SparkSession.builder\n",
    ".appName('SparkRAPIDS')\n",
    ".config('spark.plugins','com.nvidia.spark.SQLPlugin')\n",
    ".config (\"spark.driver.memory\", \"64g\")\n",
    ".getOrCreate()\n",
    "        )\n",
    "spark.sparkContext.addPyFile(rapids_jar_path)\n",
    "spark.sparkContext.addPyFile(cudf_jar_path)\n",
    "spark.conf.set('spark.rapids.sql.enabled','true')\n",
    "spark.conf.set('spark.rapids.sql.incompatibleOps.enabled', 'true')\n",
    "spark.conf.set('spark.rapids.sql.format.csv.read.enabled', 'true')\n",
    "spark.conf.set('spark.rapids.sql.format.csv.enabled', 'true')\n",
    "\n",
    "\n",
    "spark.conf.set('spark.rapids.sql.format.parquet.read.enabled', 'true')\n",
    "spark.conf.set('spark.rapids.sql.format.parquet.enabled', 'true')\n",
    "spark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "print('Hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PuV9CWoMN60A"
   },
   "source": [
    "## III- Dataframe preparing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Em4ZWvqhPoHt"
   },
   "source": [
    "### 1. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "xMI_45McvZto"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "#df = spark.read.csv(\"input/news.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"input\"\n",
    "df = spark.read.load(os.path.join(path, 'news.parquet'), format='parquet', sep=',', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x4csdl2FOVf8"
   },
   "source": [
    "### 2. Partition and cache the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4EXrJ3GIBByT",
    "outputId": "0d866061-6fee-4bbb-fd79-84c946204d72"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "PPBefKKqBByU"
   },
   "outputs": [],
   "source": [
    "num_partitions=4*40\n",
    "df= df.repartition(num_partitions).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ytFY1tjRBByU",
    "outputId": "cf5d7480-394e-43ac-e8e0-bc5cef64d4e2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4G5XozIiQTEx"
   },
   "source": [
    "### 3. Preview the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r86OcjwLv0EU",
    "outputId": "63e295fa-1b60-414b-f44f-3f8c8fd24a4b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1716608"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sfy_ivN4BByV",
    "outputId": "c374612e-8073-411f-a842-736cbe7eef50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+\n",
      "|category_label|description_filtered|\n",
      "+--------------+--------------------+\n",
      "|           0.0|10 leadership les...|\n",
      "|           0.0|12000page charge ...|\n",
      "|           0.0|201819 premium co...|\n",
      "|           0.0|3 bank failed tak...|\n",
      "|           0.0|4 investing lesso...|\n",
      "|           0.0|5 tip successful ...|\n",
      "|           0.0|7 reason starting...|\n",
      "|           0.0|abbott india q1 n...|\n",
      "|           0.0|according rbi dat...|\n",
      "|           0.0|acute kidney inju...|\n",
      "|           0.0|affect home auto ...|\n",
      "|           0.0|ahmedabadbased sh...|\n",
      "|           0.0|airtel partnering...|\n",
      "|           0.0|allahabad bankled...|\n",
      "|           0.0|although bank cre...|\n",
      "|           0.0|america fastest g...|\n",
      "|           0.0|amid speculation ...|\n",
      "|           0.0|andhra bank poste...|\n",
      "|           0.0|another reason st...|\n",
      "|           0.0|application ha re...|\n",
      "+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "awWri7zZQcOl",
    "outputId": "80402c12-f32a-483f-9ed0-ddad43eda4c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- category_label: double (nullable = true)\n",
      " |-- description_filtered: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VMpXsNNsO_Z6"
   },
   "source": [
    "### 4. Convert filtered descriptions to arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Drqnhee0i26r",
    "outputId": "63a0cd7d-df8f-45c9-a6b5-ef7cf9bf7ebc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|category_label|description_filtered                                                                                                                       |\n",
      "+--------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0.0           |[10, leadership, lesson, u, commanding, general, john, e, michel]                                                                          |\n",
      "|0.0           |[12000page, charge, sheet, claimed, similar, fraud, wa, detected, 2016, following, rbi, took, issue]                                       |\n",
      "|0.0           |[201819, premium, collected, member, bank, wa, r, 12040, crore, commercial, bank, contributed, 93, per, cent]                              |\n",
      "|0.0           |[3, bank, failed, take, timely, remedial, step, aggravated, seriousness, contravention, impact, rbi, said]                                 |\n",
      "|0.0           |[4, investing, lesson, learn, warren, buffett]                                                                                             |\n",
      "|0.0           |[5, tip, successful, content, marketing]                                                                                                   |\n",
      "|0.0           |[7, reason, starting, business, better, therapy]                                                                                           |\n",
      "|0.0           |[abbott, india, q1, net, profit, jump, 54, r, 18035, crore]                                                                                |\n",
      "|0.0           |[according, rbi, data, lender, 672, mn, outstanding, card, october, highest, industry]                                                     |\n",
      "|0.0           |[acute, kidney, injury, treatment, market, research, report, type, treatment, enduser, global, forecast, 2025, cumulative, impact, covid19]|\n",
      "|0.0           |[affect, home, auto, loan, buyer, state, bank, india, sbi, today, said, plan, hike, base, rate, december]                                  |\n",
      "|0.0           |[ahmedabadbased, shri, mahila, sewa, sahakari, bank, ltd, microfinance, wing, self, employed, woman, association, sewa, aim]               |\n",
      "|0.0           |[airtel, partnering, standard, chartered, bank, expands, fintech, business]                                                                |\n",
      "|0.0           |[allahabad, bankled, fiveparty, venture, nonlife, insurance, company, expected, start, operation, aprilmay, 2007]                          |\n",
      "|0.0           |[although, bank, credit, increased, r, 21460, crore, fortnight, ended, june, 5, 2009, falling, r, 16306, crore]                            |\n",
      "|0.0           |[america, fastest, growing, shrinking, economy, 247, wall, st]                                                                             |\n",
      "|0.0           |[amid, speculation, reserve, bank, india, rbi, may, buy, gold, imf, prime, minister, economic]                                             |\n",
      "|0.0           |[andhra, bank, posted, marginal, 575, per, cent, rise, net, profit, r, 13628, crore, quarter, ended, december, 2006, r]                    |\n",
      "|0.0           |[another, reason, standing, desk, user, feel, smug]                                                                                        |\n",
      "|0.0           |[application, ha, returned, reason, application, fee, also, returned]                                                                      |\n",
      "+--------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a new DataFrame with description_filtered as arrays\n",
    "df= df.withColumn('description_filtered', split(col('description_filtered'), ' '))\n",
    "# Show the new DataFrame\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3RmBazOpQqXF"
   },
   "source": [
    "## IV- Feature Engineering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LjsTG0YLSyEn"
   },
   "source": [
    "### 1. Explode the filtered descriptions to get the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4qON2vCKBByX",
    "outputId": "3b8af783-1259-46dd-9346-ee5ee60033c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|       col|\n",
      "+----------+\n",
      "|        10|\n",
      "|leadership|\n",
      "|    lesson|\n",
      "|         u|\n",
      "|commanding|\n",
      "|   general|\n",
      "|      john|\n",
      "|         e|\n",
      "|    michel|\n",
      "| 12000page|\n",
      "|    charge|\n",
      "|     sheet|\n",
      "|   claimed|\n",
      "|   similar|\n",
      "|     fraud|\n",
      "|        wa|\n",
      "|  detected|\n",
      "|      2016|\n",
      "| following|\n",
      "|       rbi|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exploded_df=df.select(explode(df.description_filtered)).alias('words')\n",
    "exploded_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "DH41jOsrBByX"
   },
   "outputs": [],
   "source": [
    "#df=df.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MohTRQirTXD3"
   },
   "source": [
    "### 2. Get unique words in the filtered_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "2NbJFyQlBByX"
   },
   "outputs": [],
   "source": [
    "unique_words=exploded_df.distinct()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-UjxthKU0Rq"
   },
   "source": [
    "### 3. Cache and show the unique words dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h1S8h39xBByX",
    "outputId": "8c884387-1347-4441-d313-56027086df18"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:=========================================>           (126 + 34) / 160]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|        col|\n",
      "+-----------+\n",
      "|     36goal|\n",
      "|        isp|\n",
      "|    quatern|\n",
      "|      2015a|\n",
      "| onetimeuse|\n",
      "|        467|\n",
      "|    chairez|\n",
      "|      hobbs|\n",
      "|   linguini|\n",
      "|   antilent|\n",
      "|   monkfish|\n",
      "|       lwor|\n",
      "|       cbis|\n",
      "|       koxa|\n",
      "|   tortured|\n",
      "|        kkr|\n",
      "|     online|\n",
      "| trezeguets|\n",
      "|    nimbler|\n",
      "|paracycling|\n",
      "+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "unique_words=unique_words.cache()\n",
    "unique_words.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KCb1pLl3VLOL"
   },
   "source": [
    "### 4. Get the vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dCakXZ3PBByX",
    "outputId": "e54847cd-6dce-4cf7-ffcb-7770061fcf18"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128622"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size=unique_words.count()\n",
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cV9aikKbYQzj"
   },
   "source": [
    "### 5. Define the CountVectorizer and IDF stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qYJp-K7O8pWN",
    "outputId": "dbdeed43-bf9e-40c4-ee66-ecd519909310"
   },
   "outputs": [],
   "source": [
    "# Define the HashingTF and IDF stages\n",
    "vectorizer = CountVectorizer(inputCol=\"description_filtered\", outputCol=\"raw_features\",vocabSize=vocabulary_size, minDF=3.0)\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74T1zTMRxKP8"
   },
   "source": [
    "## V- Models set up, training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HmY4gqj5Q2w_"
   },
   "source": [
    "### 1. Set up LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "f8ya3gJ_PxLc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LDA_4a2474afb468"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#num_topics = 20\n",
    "#lda = LDA(k=num_topics, maxIter=10)\n",
    "lda = LDA(featuresCol=\"features\",seed=0)\n",
    "lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XT81OlVr5Ck1"
   },
   "source": [
    "### 2. Set up pipelines\n",
    "\n",
    "We will  set up the pipelines of the following transformations for Naive Bayes and Linear reggression\n",
    "\n",
    "- CountVectorizer\n",
    "- IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline_9a379cbe3a44"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import CountVectorizer, IDF\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.ml.clustering import LDA\n",
    " \n",
    "# Create pipeline for LDA\n",
    "pipeline = Pipeline(stages=[vectorizer, idf, lda]) \n",
    "\n",
    "\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UD0gwMNf7yY7"
   },
   "source": [
    "### 3. Split the data\n",
    "\n",
    "First of all let us split the data into train and test set: 80% for train and 20% for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "pThnAKRy8LqP"
   },
   "outputs": [],
   "source": [
    "# Split data\n",
    "(train_set, test_set) = df.randomSplit([0.8, 0.2], seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M84xeE8i67YF"
   },
   "source": [
    "### 4. Create a function for model training\n",
    "\n",
    "Let us create a function which takes as argument a model that it trains and then returns the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AvRF-E1-7ncY",
    "outputId": "a361d112-8d8c-43c1-c926-d4a562e61afd"
   },
   "outputs": [],
   "source": [
    "def train_model(model):    \n",
    "    return model.fit(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/08 01:46:28 WARN DAGScheduler: Broadcasting large task binary with size 1974.4 KiB\n",
      "24/06/08 01:46:30 WARN DAGScheduler: Broadcasting large task binary with size 1974.5 KiB\n",
      "24/06/08 01:46:31 WARN DAGScheduler: Broadcasting large task binary with size 1991.3 KiB\n",
      "24/06/08 01:46:31 WARN DAGScheduler: Broadcasting large task binary with size 13.2 MiB\n",
      "24/06/08 01:46:32 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "24/06/08 01:46:32 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "24/06/08 01:46:35 WARN DAGScheduler: Broadcasting large task binary with size 18.8 MiB\n",
      "24/06/08 01:46:36 WARN DAGScheduler: Broadcasting large task binary with size 1991.3 KiB\n",
      "24/06/08 01:46:36 WARN DAGScheduler: Broadcasting large task binary with size 13.2 MiB\n",
      "24/06/08 01:46:38 WARN DAGScheduler: Broadcasting large task binary with size 18.8 MiB\n",
      "24/06/08 01:46:38 WARN DAGScheduler: Broadcasting large task binary with size 1991.3 KiB\n",
      "24/06/08 01:46:39 WARN DAGScheduler: Broadcasting large task binary with size 13.2 MiB\n",
      "24/06/08 01:46:40 WARN DAGScheduler: Broadcasting large task binary with size 18.8 MiB\n",
      "24/06/08 01:46:40 WARN DAGScheduler: Broadcasting large task binary with size 1991.3 KiB\n",
      "24/06/08 01:46:41 WARN DAGScheduler: Broadcasting large task binary with size 13.2 MiB\n",
      "24/06/08 01:46:42 WARN DAGScheduler: Broadcasting large task binary with size 18.8 MiB\n",
      "24/06/08 01:46:42 WARN DAGScheduler: Broadcasting large task binary with size 1991.3 KiB\n",
      "24/06/08 01:46:42 WARN DAGScheduler: Broadcasting large task binary with size 13.2 MiB\n",
      "24/06/08 01:46:44 WARN DAGScheduler: Broadcasting large task binary with size 18.8 MiB\n",
      "24/06/08 01:46:44 WARN DAGScheduler: Broadcasting large task binary with size 1991.3 KiB\n",
      "24/06/08 01:46:45 WARN DAGScheduler: Broadcasting large task binary with size 13.2 MiB\n",
      "24/06/08 01:46:46 WARN DAGScheduler: Broadcasting large task binary with size 18.8 MiB\n",
      "24/06/08 01:46:46 WARN DAGScheduler: Broadcasting large task binary with size 1991.3 KiB\n",
      "24/06/08 01:46:47 WARN DAGScheduler: Broadcasting large task binary with size 13.2 MiB\n",
      "24/06/08 01:46:48 WARN DAGScheduler: Broadcasting large task binary with size 18.8 MiB\n",
      "24/06/08 01:46:48 WARN DAGScheduler: Broadcasting large task binary with size 1991.3 KiB\n",
      "24/06/08 01:46:49 WARN DAGScheduler: Broadcasting large task binary with size 13.2 MiB\n",
      "24/06/08 01:46:50 WARN DAGScheduler: Broadcasting large task binary with size 18.8 MiB\n",
      "24/06/08 01:46:50 WARN DAGScheduler: Broadcasting large task binary with size 1991.3 KiB\n",
      "24/06/08 01:46:51 WARN DAGScheduler: Broadcasting large task binary with size 13.2 MiB\n",
      "24/06/08 01:46:52 WARN DAGScheduler: Broadcasting large task binary with size 18.8 MiB\n",
      "24/06/08 01:46:52 WARN DAGScheduler: Broadcasting large task binary with size 1991.3 KiB\n",
      "24/06/08 01:46:53 WARN DAGScheduler: Broadcasting large task binary with size 13.2 MiB\n",
      "24/06/08 01:46:54 WARN DAGScheduler: Broadcasting large task binary with size 18.8 MiB\n",
      "24/06/08 01:46:54 WARN DAGScheduler: Broadcasting large task binary with size 1991.3 KiB\n",
      "24/06/08 01:46:54 WARN DAGScheduler: Broadcasting large task binary with size 13.2 MiB\n",
      "24/06/08 01:46:56 WARN DAGScheduler: Broadcasting large task binary with size 18.8 MiB\n",
      "24/06/08 01:46:56 WARN DAGScheduler: Broadcasting large task binary with size 1991.3 KiB\n",
      "24/06/08 01:46:57 WARN DAGScheduler: Broadcasting large task binary with size 13.2 MiB\n",
      "24/06/08 01:46:58 WARN DAGScheduler: Broadcasting large task binary with size 18.8 MiB\n",
      "24/06/08 01:46:58 WARN DAGScheduler: Broadcasting large task binary with size 1991.3 KiB\n",
      "24/06/08 01:46:59 WARN DAGScheduler: Broadcasting large task binary with size 13.2 MiB\n",
      "24/06/08 01:47:00 WARN DAGScheduler: Broadcasting large task binary with size 18.8 MiB\n",
      "24/06/08 01:47:00 WARN DAGScheduler: Broadcasting large task binary with size 1991.3 KiB\n",
      "24/06/08 01:47:00 WARN DAGScheduler: Broadcasting large task binary with size 13.2 MiB\n",
      "24/06/08 01:47:01 WARN DAGScheduler: Broadcasting large task binary with size 18.8 MiB\n",
      "24/06/08 01:47:02 WARN DAGScheduler: Broadcasting large task binary with size 1991.3 KiB\n",
      "24/06/08 01:47:02 WARN DAGScheduler: Broadcasting large task binary with size 13.2 MiB\n",
      "24/06/08 01:47:03 WARN DAGScheduler: Broadcasting large task binary with size 18.8 MiB\n",
      "24/06/08 01:47:04 WARN DAGScheduler: Broadcasting large task binary with size 1991.3 KiB\n",
      "24/06/08 01:47:04 WARN DAGScheduler: Broadcasting large task binary with size 13.2 MiB\n",
      "24/06/08 01:47:05 WARN DAGScheduler: Broadcasting large task binary with size 18.8 MiB\n",
      "24/06/08 01:47:06 WARN DAGScheduler: Broadcasting large task binary with size 1991.3 KiB\n",
      "24/06/08 01:47:06 WARN DAGScheduler: Broadcasting large task binary with size 13.2 MiB\n",
      "24/06/08 01:47:07 WARN DAGScheduler: Broadcasting large task binary with size 18.8 MiB\n",
      "24/06/08 01:47:07 WARN DAGScheduler: Broadcasting large task binary with size 1991.3 KiB\n",
      "24/06/08 01:47:08 WARN DAGScheduler: Broadcasting large task binary with size 13.2 MiB\n",
      "24/06/08 01:47:09 WARN DAGScheduler: Broadcasting large task binary with size 18.8 MiB\n",
      "24/06/08 01:47:09 WARN DAGScheduler: Broadcasting large task binary with size 1991.3 KiB\n",
      "24/06/08 01:47:09 WARN DAGScheduler: Broadcasting large task binary with size 13.2 MiB\n",
      "24/06/08 01:47:11 WARN DAGScheduler: Broadcasting large task binary with size 18.8 MiB\n",
      "24/06/08 01:47:12 WARN DAGScheduler: Broadcasting large task binary with size 1991.3 KiB\n",
      "24/06/08 01:47:12 WARN DAGScheduler: Broadcasting large task binary with size 13.2 MiB\n",
      "24/06/08 01:47:13 WARN DAGScheduler: Broadcasting large task binary with size 18.8 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PipelineModel_71ce7aaf14f4"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitted_model=train_model(pipeline)\n",
    "fitted_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Visualize the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73468"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitted_vectirizer=fitted_model.stages[0]\n",
    "vocabulary= fitted_vectirizer.vocabulary\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['new', 'photo', 'state', 'trump', 'day', 'nt', 'say', 'woman', 'make', 'get']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|topic|         termIndices|         termWeights|\n",
      "+-----+--------------------+--------------------+\n",
      "|    0|[191, 2, 7, 306, ...|[0.00386515694159...|\n",
      "|    1|[50, 1, 74, 4, 32...|[0.00322483897564...|\n",
      "|    2|[35, 11, 14, 1, 5...|[0.00463661178938...|\n",
      "|    3|[5, 19, 41, 208, ...|[0.00345376318725...|\n",
      "|    4|[2, 202, 0, 340, ...|[0.00387164342803...|\n",
      "|    5|[21, 192, 92, 13,...|[0.00760989750144...|\n",
      "|    6|[3, 36, 231, 339,...|[0.00656038181206...|\n",
      "|    7|[9, 39, 266, 51, ...|[0.00321040978364...|\n",
      "|    8|[26, 4, 0, 111, 7...|[0.00527198922910...|\n",
      "|    9|[16, 30, 0, 55, 5...|[0.00680606310236...|\n",
      "+-----+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saturne.ayidegnon/spark-3.2.0/python/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "topics = fitted_model.stages[-1].describeTopics()   \n",
    "topics.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['medium',\n",
       "  'state',\n",
       "  'woman',\n",
       "  'social',\n",
       "  'market',\n",
       "  'republic',\n",
       "  'global',\n",
       "  'young',\n",
       "  'new',\n",
       "  'baby'],\n",
       " ['family',\n",
       "  'photo',\n",
       "  'food',\n",
       "  'day',\n",
       "  'best',\n",
       "  'angstrom',\n",
       "  'new',\n",
       "  'relationship',\n",
       "  'unit',\n",
       "  'clarence']]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_rdd = topics.rdd\n",
    "topics_words = topics_rdd\\\n",
    "       .map(lambda row: row['termIndices'])\\\n",
    "       .map(lambda idx_list: [vocabulary[idx] for idx in idx_list])\\\n",
    "       .collect()\n",
    "topics_words[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic: 0\n",
      "*************************\n",
      "medium\n",
      "state\n",
      "woman\n",
      "social\n",
      "market\n",
      "republic\n",
      "global\n",
      "young\n",
      "new\n",
      "baby\n",
      "*************************\n",
      "topic: 1\n",
      "*************************\n",
      "family\n",
      "photo\n",
      "food\n",
      "day\n",
      "best\n",
      "angstrom\n",
      "new\n",
      "relationship\n",
      "unit\n",
      "clarence\n",
      "*************************\n",
      "topic: 2\n",
      "*************************\n",
      "police\n",
      "video\n",
      "5\n",
      "photo\n",
      "marriage\n",
      "man\n",
      "new\n",
      "force\n",
      "show\n",
      "picture\n",
      "*************************\n",
      "topic: 3\n",
      "*************************\n",
      "nt\n",
      "world\n",
      "health\n",
      "organization\n",
      "new\n",
      "trump\n",
      "state\n",
      "school\n",
      "u\n",
      "make\n",
      "*************************\n",
      "topic: 4\n",
      "*************************\n",
      "state\n",
      "tree\n",
      "new\n",
      "race\n",
      "right\n",
      "bill\n",
      "concern\n",
      "say\n",
      "5\n",
      "life\n",
      "*************************\n",
      "topic: 5\n",
      "*************************\n",
      "bank\n",
      "r\n",
      "india\n",
      "ha\n",
      "space\n",
      "said\n",
      "state\n",
      "crore\n",
      "nt\n",
      "20\n",
      "*************************\n",
      "topic: 6\n",
      "*************************\n",
      "trump\n",
      "donald\n",
      "clinton\n",
      "hillary\n",
      "best\n",
      "new\n",
      "nt\n",
      "card\n",
      "say\n",
      "m\n",
      "*************************\n",
      "topic: 7\n",
      "*************************\n",
      "get\n",
      "love\n",
      "room\n",
      "black\n",
      "woman\n",
      "photo\n",
      "nt\n",
      "way\n",
      "sex\n",
      "video\n",
      "*************************\n",
      "topic: 8\n",
      "*************************\n",
      "wedding\n",
      "day\n",
      "new\n",
      "hour\n",
      "college\n",
      "sexual\n",
      "parent\n",
      "photo\n",
      "gift\n",
      "factor\n",
      "*************************\n",
      "topic: 9\n",
      "*************************\n",
      "covid19\n",
      "coronavirus\n",
      "new\n",
      "case\n",
      "news\n",
      "day\n",
      "get\n",
      "u\n",
      "death\n",
      "test\n",
      "*************************\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in enumerate(topics_words):\n",
    "    print(\"topic: {}\".format(idx))\n",
    "    print(\"*\"*25)\n",
    "    for word in topic:\n",
    "       print(word)\n",
    "    print(\"*\"*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get topics distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the training and test data\n",
    "train_set_transformed = fitted_model.transform(train_set)\n",
    "test_set_transformed = fitted_model.transform(test_set)\n",
    "\n",
    "# Get the LDA model from the pipeline model\n",
    "lda_model = fitted_model.stages[-1]\n",
    "\n",
    "# Extract the topic distributions\n",
    "train_topic_distributions = train_set_transformed.select(\"description_filtered\", \"topicDistribution\")\n",
    "test_topic_distributions = test_set_transformed.select(\"description_filtered\", \"topicDistribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/08 01:47:18 WARN DAGScheduler: Broadcasting large task binary with size 7.5 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|description_filtered                                                                                                                       |topicDistribution                                                                                                                                                                                                      |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[10, leadership, lesson, u, commanding, general, john, e, michel]                                                                          |[0.0014933548385454702,0.0014966526516422274,0.0015090442645415384,0.46988078012300283,0.0015093030866906048,0.5180648252209521,0.0015100959817648857,0.0015002258584187267,0.0015122402854578388,0.001523477688983919]|\n",
      "|[12000page, charge, sheet, claimed, similar, fraud, wa, detected, 2016, following, rbi, took, issue]                                       |[0.0010965617081845455,0.22835277868529055,0.0011080434873728005,0.0011009673867560918,0.623914760925011,0.001100390771439506,0.001108936207862937,0.0011016451997580721,0.13999723192177738,0.0011186837065467019]    |\n",
      "|[201819, premium, collected, member, bank, wa, r, 12040, crore, commercial, bank, contributed, 93, per, cent]                              |[9.191249925970462E-4,9.211568633988769E-4,9.287748977465119E-4,9.22838993737953E-4,9.28924541972888E-4,0.9916579386617441,9.294323606150198E-4,9.233687597951682E-4,9.307408067361345E-4,9.37699121656118E-4]         |\n",
      "|[3, bank, failed, take, timely, remedial, step, aggravated, seriousness, contravention, impact, rbi, said]                                 |[9.109400408780628E-4,0.12331256129552609,9.20404265557456E-4,9.145584213719152E-4,9.205438443055716E-4,0.3518288983247109,0.12412423919824937,9.150291475113568E-4,0.14832300780106458,0.2478298176608247]            |\n",
      "|[4, investing, lesson, learn, warren, buffett]                                                                                             |[0.002150142256842076,0.002154941598750156,0.19674271128324078,0.002158742545244947,0.7859304000749538,0.002157644712033673,0.00217431963912612,0.0021600546462191743,0.002177398469640828,0.002193644773948362]       |\n",
      "|[5, tip, successful, content, marketing]                                                                                                   |[0.002772112313167992,0.002778360810408735,0.0028013443516239703,0.002783162682480326,0.4487199295850035,0.0027817169653050846,0.002803227503392535,0.002784919337870138,0.5289473033096902,0.0028279231410576634]     |\n",
      "|[7, reason, starting, business, better, therapy]                                                                                           |[0.002533707880189003,0.0025393933849149936,0.002560378284472843,0.002543893939905833,0.0025607091768493477,0.00254257189804642,0.0025622147858353813,0.0025454534307713965,0.002565886199713844,0.977045791019301]    |\n",
      "|[abbott, india, q1, net, profit, jump, 54, r, 18035, crore]                                                                                |[0.001396927590587709,0.001400123474241316,0.0014116306498692611,0.0014025148834069274,0.0014117891380405856,0.837698664335915,0.15103529664424256,0.001403364077638687,0.0014145624785943524,0.001425126727463795]    |\n",
      "|[according, rbi, data, lender, 672, mn, outstanding, card, october, highest, industry]                                                     |[0.0011044249013050958,0.0011068764564706078,0.12496265595541955,0.0011088271596800133,0.0011162053545518476,0.7156798425061417,0.0011168375960963987,0.001109496298855933,0.0011183673974197181,0.15157646637405917]  |\n",
      "|[acute, kidney, injury, treatment, market, research, report, type, treatment, enduser, global, forecast, 2025, cumulative, impact, covid19]|[0.7773294279608032,8.010932756718531E-4,8.076891671582179E-4,8.025293346117713E-4,8.078155404642097E-4,8.020720944426011E-4,8.082675851174233E-4,8.029792087913679E-4,8.094179362188403E-4,0.21622870789672066]       |\n",
      "|[affect, home, auto, loan, buyer, state, bank, india, sbi, today, said, plan, hike, base, rate, december]                                  |[9.313587025731004E-4,9.333582073416039E-4,9.411040103195072E-4,9.350345793512364E-4,9.4122985242535E-4,0.9318146731656242,9.417664158739989E-4,9.356472729051157E-4,9.430652576409262E-4,0.06068276253594491]         |\n",
      "|[ahmedabadbased, shri, mahila, sewa, sahakari, bank, ltd, microfinance, wing, self, employed, woman, association, sewa, aim]               |[6.947781789314597E-4,6.962834923181872E-4,7.02087093116663E-4,6.975284403320496E-4,7.022061790248632E-4,0.46318546348866224,0.06752700933728735,0.4643822868835678,7.035372889757582E-4,7.088196177835678E-4]         |\n",
      "|[airtel, partnering, standard, chartered, bank, expands, fintech, business]                                                                |[0.0013541155642752339,0.001357116654649108,0.0013683312637985988,0.16653139898733563,0.6911488728599956,0.0013589068185497007,0.001369290832366391,0.13275930402894293,0.0013712136022574677,0.0013814493878293646]   |\n",
      "|[allahabad, bankled, fiveparty, venture, nonlife, insurance, company, expected, start, operation, aprilmay, 2007]                          |[9.82351845580748E-4,9.844843443576963E-4,9.92646031088057E-4,9.86244411785734E-4,9.928132536429438E-4,0.9910843412183835,9.933325164945906E-4,9.869078716572364E-4,9.94741671124489E-4,0.0010021368358850266]         |\n",
      "|[although, bank, credit, increased, r, 21460, crore, fortnight, ended, june, 5, 2009, falling, r, 16306, crore]                            |[9.488590740622204E-4,9.509514577731555E-4,9.588414006774196E-4,9.52661379803109E-4,9.589799234826157E-4,0.9913880471922986,9.595074990190509E-4,9.533072962586232E-4,9.60846348108046E-4,9.679984285171208E-4]        |\n",
      "|[america, fastest, growing, shrinking, economy, 247, wall, st]                                                                             |[0.0015434088855095058,0.21274050855845858,0.0015596024957195687,0.5052745133684334,0.06255215708844303,0.0015487818885005149,0.0015606892867994187,0.2100829593044289,0.001562867753884827,0.0015745113698221365]     |\n",
      "|[amid, speculation, reserve, bank, india, rbi, may, buy, gold, imf, prime, minister, economic]                                             |[0.0011189597023243383,0.0011214197380421808,0.11396398497754151,0.0011234222146750239,0.0011308645666603797,0.7652233994771925,0.001131474723236466,0.0011240860640838238,0.11292084288630487,0.0011415456499391113]  |\n",
      "|[andhra, bank, posted, marginal, 575, per, cent, rise, net, profit, r, 13628, crore, quarter, ended, december, 2006, r]                    |[7.525504289157339E-4,7.542159349280216E-4,7.604522466861852E-4,0.05750976661561899,7.605544442951398E-4,0.9364156604608043,7.609899184347534E-4,7.560144916933621E-4,7.620525989326503E-4,7.677428596909783E-4]       |\n",
      "|[another, reason, standing, desk, user, feel, smug]                                                                                        |[0.001753666066955665,0.001757748337305674,0.001772268326883975,0.5989011574952672,0.0017724236950732155,0.001759786872834566,0.0017734532664328026,0.001761883295322374,0.3869583891844389,0.0017892234594856827]     |\n",
      "|[application, ha, returned, reason, application, fee, also, returned]                                                                      |[0.7481490479358337,0.0015998998167861425,0.08881451295539107,0.00160274004709249,0.0016133872010752157,0.15175705055248553,0.0016142712549085802,0.0016037559606950085,0.0016165920847730903,0.0016287421909593927]   |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|description_filtered                                                                                                                                                  |topicDistribution                                                                                                                                                                                                      |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[arrested, auditor, m, k, sharma, wa, allegedly, responsible, auditing, system, practice, bank, brady, house]                                                         |[0.1899857769199242,9.382530954296721E-4,9.460403719047125E-4,0.2421398859082385,9.462011227146511E-4,0.3741420020793724,0.18805807948311734,9.406076103188382E-4,9.480117467823501E-4,9.551416621974197E-4]           |\n",
      "|[asian, stock, set, mixed, start, u, share, rise]                                                                                                                     |[0.0018583724429811696,0.001862430732665834,0.0018780106614245114,0.001865814926709536,0.6121394548834111,0.37287208988956805,0.0018792251839714518,0.001866910974028476,0.0018817783595758044,0.0018959119456641493]  |\n",
      "|[bank, anticipating, difficulty, adhering, reserve, bank, india, rbi, guideline, valuation, property, want, model]                                                    |[9.700655421756343E-4,9.722037118254557E-4,9.802338835491404E-4,9.739423061405805E-4,0.1725881471577837,0.34863300326029256,9.809384347275654E-4,9.745389598232488E-4,0.47193712705426394,9.89799689418108E-4]         |\n",
      "|[bank, baroda, ltd, india, secondbiggest, staterun, lender, asset, posted, friday, unexpectedly, sharp, 89, tumble]                                                   |[8.2168548694791E-4,8.235355655900532E-4,8.303117731870361E-4,8.24967347615092E-4,8.304272157915881E-4,0.9925423296409015,8.308883142538315E-4,8.25515298189923E-4,8.320561255023844E-4,8.382832320207392E-4]          |\n",
      "|[bank, currently, collecting, data, loan, due, providing, information, government]                                                                                    |[0.0014731756848665484,0.16365530981083454,0.0014885522763243948,0.0014790664303856666,0.46858019472185386,0.23007864800617958,0.0014896741831967259,0.0014799023670103795,0.12877256114644633,0.001502915372902039]   |\n",
      "|[bank, result, chairman, managing, director, k, r, kamath, said, increase, net, profit, wa, due, fall]                                                                |[9.483379760941098E-4,9.504315800564588E-4,9.582902742092378E-4,9.521879579276747E-4,9.584449369719111E-4,0.9913928511304921,9.589629949645632E-4,9.526970736842081E-4,9.603216033540544E-4,9.674744722454391E-4]      |\n",
      "|[bank, suffer, high, default, rate, 40, per, cent, rural, area, 10, per, cent, nongovernmental, agency]                                                               |[0.13558521930273845,9.417993936968143E-4,9.495052547921463E-4,9.434319013482689E-4,9.496892845932158E-4,0.8568260327810833,9.50191848516708E-4,9.439698766160806E-4,9.514981312387956E-4,9.586622253761712E-4]        |\n",
      "|[banking, service, including, branch, operation, treasury, operation, hit, hard, staff, officer, belonging, bank]                                                     |[0.001053731336501587,0.001056015229348093,0.0010647902677216876,0.0010578747524832473,0.0010649166011150674,0.8894192718563697,0.0010655569356991762,0.10207580174235423,0.001067046165662079,0.0010749951127453175]  |\n",
      "|[brookfield, raise, us23b, expects, ramp, pace, deal]                                                                                                                 |[0.0019333171823806281,0.0019375881025227681,0.27166508908837544,0.001941028773065177,0.001953877445885292,0.7127417978389597,0.0019549800241764545,0.0019422440040005894,0.0019576959389956726,0.001972381601638216]  |\n",
      "|[capital, market, regulator, sebi, ha, decided, share, ministry, corporate, affair, name, 500, company]                                                               |[0.0010792103963992497,0.001081560075736792,0.0010904976901155345,0.001083529683240906,0.07202816871254145,0.8247651812250523,0.0010913254827766494,0.0010841337786526547,0.001092810241798752,0.0956035827136859]     |\n",
      "|[chinese, executive, joining, u, president, barack, obama, backing, stronger, yuan, even, premier, wen, jiabao, say, currency]                                        |[7.716826684039833E-4,0.10655156772415852,7.797645757427898E-4,7.747918228203795E-4,0.4329812592424455,0.4550166344016892,7.803649214225938E-4,7.752679727093794E-4,7.814102604738946E-4,7.872564101337691E-4]         |\n",
      "|[citibank, keen, lead, consortium, lender, ulta, mega, power, project, umpps, bridging, working, capital]                                                             |[9.542706842311853E-4,0.1403784152654035,0.10623131219385068,9.580732670351535E-4,0.13441342953893343,0.10694820476427021,9.64916069071829E-4,0.33255455609513124,9.662893875117858E-4,0.1756305327345608]             |\n",
      "|[citybased, mediciti, hospital, national, insurance, company, come, together, offer, new, health, plan, cover, existing]                                              |[0.00116072745465144,0.0011632723509344878,0.0011729357592314593,0.0011654804147832193,0.23251857384711383,0.7049102913175385,0.0011737394037912804,0.0011661049004065213,0.0011754082205500808,0.05439346633099901]   |\n",
      "|[commerzbank, raised, thirdquarter, net, profit, half, thanks, oneoff, german, tax, gain, missed, expectation, gain, wa, marred, big, writedown, subprime, investment]|[0.0713781147542177,5.692237378523007E-4,5.738959215433488E-4,5.702074090817462E-4,0.44677207210096875,0.3082307270767063,0.09121389427572174,5.705619880026781E-4,5.751153768888836E-4,0.07954618735901654]           |\n",
      "|[country, foreign, exchange, reserve, rose, 316, billion, week, ended, december, 19, 320, billion, according, data]                                                   |[8.569267137242982E-4,8.588277153526472E-4,8.659436519939402E-4,8.603531620432294E-4,0.3864736839980564,0.4846331945950149,8.665333698460695E-4,0.12284246411250792,8.677750881395353E-4,8.742975933208991E-4]         |\n",
      "|[covid19, vaccine, within, 6, month]                                                                                                                                  |[0.0032258137058317803,0.00323301150149563,0.003259613225654249,0.0032386560336215573,0.003260053758228096,0.295821195984007,0.0032621505584403613,0.003240918097892656,0.0032665849851572994,0.6781920021496715]      |\n",
      "|[diversified, financial, service, structure, way, go, rashesh, shah]                                                                                                  |[0.0018601199285965078,0.3129032807256902,0.0018796577165842761,0.0018677088274530193,0.0018799510784099885,0.672078084202298,0.0018810391469947702,0.0018688633981014282,0.0018836495935787676,0.0018976453822930218] |\n",
      "|[drew, parallel, weak, recapitalisation, european, bank]                                                                                                              |[0.001885988914453898,0.15593585690487538,0.0019060225784321618,0.0018936622486308348,0.0019061301619718095,0.26060417574266237,0.0019071226253693202,0.5701270245076663,0.0019098340229222223,0.0019241822930157815]  |\n",
      "|[earlier, week, government, decided, infuse, r, 5042, crore, bob, enhance, capital, base, lender, ahead]                                                              |[0.0010379509810969317,0.001040228288433837,0.08729846170988236,0.0010421119739167472,0.0010490534065222966,0.7133032473133329,0.001049590168598358,0.0010427221532387212,0.07450298036464639,0.11863365364033156]     |\n",
      "|[eye, bitcoin, another, crypto, 500, last, yearand, still, soaring]                                                                                                   |[0.0015262414679483637,0.0015296542540123759,0.0015423485430621732,0.001532375099913676,0.0015425355193538874,0.0015316184759835124,0.0015434037019746976,0.5086125071782953,0.0015456049190130821,0.47909371084044294]|\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/08 01:47:19 WARN DAGScheduler: Broadcasting large task binary with size 7.5 MiB\n"
     ]
    }
   ],
   "source": [
    "# Show the topic distributions for the training set\n",
    "train_topic_distributions.show(truncate=False)\n",
    "\n",
    "# Show the topic distributions for the test set\n",
    "test_topic_distributions.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K-fBEB1f9aRD"
   },
   "source": [
    "### 5. Define a function to evaluate the model\n",
    "\n",
    "The function takes as parameter a fitted model, evaluates the model on train and test split and then return the train and test performance. The accuracy is the metric used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate model and get best parameters\n",
    "def evaluate_model(fitted_model,data_transformed=[train_set_transformed,test_set_transformed]):\n",
    "    \n",
    "    print('Evaluating the model on training set')\n",
    "    train_lp = fitted_model.logPerplexity(data_transformed[0])\n",
    "\n",
    "    print('Evaluating the model on test set')\n",
    "    test_lp = fitted_model.logPerplexity(data_transformed[1])\n",
    "    \n",
    "    print(\"The upper bound on perplexity for train set: \" + str(train_lp))\n",
    "    print(\"The upper bound on perplexity for test set: \" + str(test_lp))\n",
    "    return train_lp, test_lp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on training set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/08 01:47:19 WARN DAGScheduler: Broadcasting large task binary with size 1975.3 KiB\n",
      "24/06/08 01:50:45 WARN DAGScheduler: Broadcasting large task binary with size 1976.0 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on test set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/08 02:14:27 WARN DAGScheduler: Broadcasting large task binary with size 1975.3 KiB\n",
      "24/06/08 02:15:20 WARN DAGScheduler: Broadcasting large task binary with size 1976.0 KiB\n",
      "[Stage 150:====================================================>(159 + 1) / 160]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The upper bound on perplexity for train set: 9.075611646425005\n",
      "The upper bound on perplexity for test set: 9.13477040948899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9.075611646425005, 9.13477040948899)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lp,test_lp=evaluate_model(fitted_model.stages[-1])\n",
    "train_lp,test_lp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results={}\n",
    "num_topics_range=[20, 25, 30, 35, 40, 45, 50]\n",
    "\n",
    "for num_topics in num_topics_range:\n",
    "    print('LDA for k={}'.format(num_topics))\n",
    "    # Create LDA\n",
    "    lda = LDA(featuresCol=\"features\",seed=0)\n",
    "    # Create pipeline for LDA\n",
    "    pipeline = Pipeline(stages=[vectorizer, idf, lda])\n",
    "    print('Model training')\n",
    "    # Train the model\n",
    "    fitted_model=train_model(pipeline)\n",
    "    print('Done')\n",
    "    \n",
    "    train_set_transformed = fitted_model.transform(train_set)\n",
    "    test_set_transformed = fitted_model.transform(test_set)\n",
    "    train_lp,test_lp=evaluate_model(fitted_model,data_transformed=[train_set_transformed,test_set_transformed])\n",
    "    results[num_topics]=\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yG6dMBjzLXWA"
   },
   "source": [
    "### 6. Create a function which takes pipelines and train the models, evaluate them and then return the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "808-IeqCNb1v"
   },
   "source": [
    "We remark that\n",
    "- Naive Bayes\n",
    "- Logistic regression\n",
    "\n",
    "We can then conclude that t\n",
    "- he two models set a good performance on both training and test set.\n",
    "- The Logistic regression models outperforms the Naive Bayes model\n",
    "\n",
    "In the next section, we will tune the parameters of the Naive bayes to get the best parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xkME6ByNOegG"
   },
   "source": [
    "## VI- Logistic regression hyperparameters tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pAWi0NqwSzgp"
   },
   "source": [
    "### 1. Pipeline creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wtq5K0JbOdpD"
   },
   "outputs": [],
   "source": [
    "# Define parameter grids for Logistic regresion grid search\n",
    "reg_values = np.logspace(-4, 4, num=100)\n",
    "l1_ratios = np.linspace(0, 1, num=10)\n",
    "\n",
    "paramGrid_lr=paramGrid_lr.addGrid(lr.regParam, reg_values).build()\n",
    "\n",
    "# Create Cross-validation for Logistic Regression\n",
    "cv_lr = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid_lr,\n",
    "                        evaluator=MulticlassClassificationEvaluator(labelCol=\"category_label\", predictionCol=\"prediction\", metricName=\"accuracy\"),\n",
    "                        numFolds=3, parallelism=1)\n",
    "\n",
    "\n",
    "# Create pipeline for Logistic Regression\n",
    "pipeline_lr = Pipeline(stages=[hashingTF, idf, cv_lr])\n",
    "\n",
    "pipeline_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0XhMJxEzS87h"
   },
   "source": [
    "### 2. Hyperparameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4bSLe3K5TL1L"
   },
   "outputs": [],
   "source": [
    "results=train_and_evaluate_models(model_pipelines=[pipeline_lr],model_names=[\"Logistic Regression\"])\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yUlLvsWVTkDy"
   },
   "source": [
    "### 3. Get the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k5rCb_boTk3e"
   },
   "outputs": [],
   "source": [
    "fitted_model=results['fitted_model']\n",
    "\n",
    "# Get the best model\n",
    "best_model = fitted_model.stages[-1].bestModel\n",
    "\n",
    "# Print the best parameters\n",
    "print(f\"Best parameters for Logistic regression:\")\n",
    "\n",
    "for param, value in best_model.extractParamMap().items():\n",
    "     print(f\"  {param.name}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uEH0OfIqUxOg"
   },
   "source": [
    "### 4. Save the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xX4pmvAVBByZ",
    "outputId": "843aa553-e419-43ba-dbb3-3ebb1a67f454"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/04 20:03:43 WARN TaskSetManager: Stage 216 contains a task of very large size (33450 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "best_model.save('output/news_categorization_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GnFsw8x5Vyy2"
   },
   "source": [
    "## VII- Summary\n",
    "\n",
    "In this notebook we have studied two models for our news categorization task. There are Naive Bayes and Logistic regression.\n",
    "\n",
    " Our study reveals that the Logistic regression was the one with best performance.\n",
    "\n",
    " Then we tunned the Logistic regression hyperparameters using grid search and then we find the best model that we save.\n",
    "\n",
    " The next step of our work will be to ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NSyD5awwBByb"
   },
   "outputs": [],
   "source": [
    "#df.unpersist()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
