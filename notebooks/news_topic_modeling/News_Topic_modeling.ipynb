{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-qFVy2lBaqu"
      },
      "source": [
        "# News Topic modeling\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNfMRWDbFXDJ"
      },
      "source": [
        "## I- Modules import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3QYNQcr-FQLs"
      },
      "outputs": [],
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.ml.feature import  IDF, HashingTF,CountVectorizer\n",
        "from pyspark.ml import  Pipeline\n",
        "from math import ceil,log2\n",
        "from pyspark.ml.classification import LogisticRegression,NaiveBayes,LogisticRegressionModel\n",
        "from pyspark.sql.functions import col,explode,split\n",
        "\n",
        "import numpy as np\n",
        "from pyspark.ml.clustering import LDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nj7Qu-EvK9_Y"
      },
      "source": [
        "## II- Spark context and session creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "aFZWn6-QBByS",
        "outputId": "cc18968f-50af-4a70-9d51-126be08a96c8",
        "scrolled": false
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://node02.cm.cluster:4041\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>spark://node15:7077</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>NewsTopicModeling</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fffb2e9ed60>"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spark = (SparkSession.builder\n",
        "    .master(\"spark://node15:7077\")\n",
        "    .appName(\"NewsTopicModeling\")\n",
        "    .getOrCreate()\n",
        "        )\n",
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuV9CWoMN60A"
      },
      "source": [
        "## III- Dataframe preparing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Em4ZWvqhPoHt"
      },
      "source": [
        "### 1. Load the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xMI_45McvZto",
        "outputId": "90225c3f-2d6b-4538-9654-f3ed34df5ad6"
      },
      "outputs": [
        {
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o264.parquet.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:834)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\n\tat org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2707)\n\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.mergeSchemasInParallel(SchemaMergeUtils.scala:63)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.mergeSchemasInParallel(ParquetFileFormat.scala:497)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$.inferSchema(ParquetUtils.scala:132)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.inferSchema(ParquetFileFormat.scala:79)\n\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:208)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:205)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:563)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "Input \u001b[0;32mIn [35]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66m# Load data\u001b[39m\n\u001b[0;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput/news.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minferSchema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/readwriter.py:544\u001b[0m, in \u001b[0;36mDataFrameReader.parquet\u001b[0;34m(self, *paths, **options)\u001b[0m\n\u001b[1;32m    533\u001b[0m int96RebaseMode \u001b[38;5;241m=\u001b[39m options\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint96RebaseMode\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[1;32m    535\u001b[0m     mergeSchema\u001b[38;5;241m=\u001b[39mmergeSchema,\n\u001b[1;32m    536\u001b[0m     pathGlobFilter\u001b[38;5;241m=\u001b[39mpathGlobFilter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m     int96RebaseMode\u001b[38;5;241m=\u001b[39mint96RebaseMode,\n\u001b[1;32m    542\u001b[0m )\n\u001b[0;32m--> 544\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o264.parquet.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:834)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\n\tat org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2707)\n\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.mergeSchemasInParallel(SchemaMergeUtils.scala:63)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.mergeSchemasInParallel(ParquetFileFormat.scala:497)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$.inferSchema(ParquetUtils.scala:132)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.inferSchema(ParquetFileFormat.scala:79)\n\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:208)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:205)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:563)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n"
          ]
        }
      ],
      "source": [
        "# Load data\n",
        "df = spark.read.parquet(\"input/news.parquet\", header=True, inferSchema=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ylheTyYylbo2"
      },
      "outputs": [],
      "source": [
        "#spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4csdl2FOVf8"
      },
      "source": [
        "### 2. Partition and cache the dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EXrJ3GIBByT",
        "outputId": "0d866061-6fee-4bbb-fd79-84c946204d72"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.rdd.getNumPartitions()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPBefKKqBByU"
      },
      "outputs": [],
      "source": [
        "num_partitions=4*20\n",
        "df= df.repartition(num_partitions).cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytFY1tjRBByU",
        "outputId": "cf5d7480-394e-43ac-e8e0-bc5cef64d4e2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 3:======================================================>  (76 + 4) / 80]\r"
          ]
        },
        {
          "data": {
            "text/plain": [
              "80"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.rdd.getNumPartitions()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4G5XozIiQTEx"
      },
      "source": [
        "### 3. Preview the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r86OcjwLv0EU",
        "outputId": "63e295fa-1b60-414b-f44f-3f8c8fd24a4b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r",
            "                                                                                \r"
          ]
        },
        {
          "data": {
            "text/plain": [
              "1716608"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfy_ivN4BByV",
        "outputId": "c374612e-8073-411f-a842-736cbe7eef50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------+--------------------+\n",
            "|category_label|description_filtered|\n",
            "+--------------+--------------------+\n",
            "|           9.0|best baby toddler...|\n",
            "|          11.0| summer luxury italy|\n",
            "|          10.0|crop top might ac...|\n",
            "|           9.0|parent reality le...|\n",
            "|          11.0|best redness past...|\n",
            "|          10.0|90 hair relaxer c...|\n",
            "|           9.0|toddler kernel pu...|\n",
            "|          11.0|london olympiad d...|\n",
            "|          11.0|resort encourage ...|\n",
            "|           9.0|challenge present...|\n",
            "|          10.0|brooke carapace w...|\n",
            "|           9.0|allhallows eve co...|\n",
            "|          10.0|ashlee wallis war...|\n",
            "|          10.0|fashion flashback...|\n",
            "|          11.0|hidden mickey spo...|\n",
            "|           9.0|trooper help deli...|\n",
            "|          11.0|chef architect di...|\n",
            "|           9.0|pricey mom prenat...|\n",
            "|          10.0|toothpaste dry pi...|\n",
            "|          11.0|work home rabbi d...|\n",
            "+--------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awWri7zZQcOl",
        "outputId": "80402c12-f32a-483f-9ed0-ddad43eda4c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- category_label: double (nullable = true)\n",
            " |-- description_filtered: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMpXsNNsO_Z6"
      },
      "source": [
        "### 4. Convert filtered descriptions to arrays"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Drqnhee0i26r",
        "outputId": "63a0cd7d-df8f-45c9-a6b5-ef7cf9bf7ebc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------+-------------------------------------------------------------------------------------------------------------------+\n",
            "|category_label|description_filtered                                                                                               |\n",
            "+--------------+-------------------------------------------------------------------------------------------------------------------+\n",
            "|9.0           |[best, baby, toddler, product, year]                                                                               |\n",
            "|11.0          |[summer, luxury, italy]                                                                                            |\n",
            "|10.0          |[crop, top, might, actually]                                                                                       |\n",
            "|9.0           |[parent, reality, leave, tyke]                                                                                     |\n",
            "|11.0          |[best, redness, pasta, sauce, italian, love, life, ve, never, heard]                                               |\n",
            "|10.0          |[90, hair, relaxer, commercial, song, stuck, head, video]                                                          |\n",
            "|9.0           |[toddler, kernel, pure, euphoria, feel, hear, favourite, sung]                                                     |\n",
            "|11.0          |[london, olympiad, day, remembrance, black, patrick, white, picture, english, capital, ahead, one, yr, mark, photo]|\n",
            "|11.0          |[resort, encourage, kid, get, incline, great, lift, ticket, price]                                                 |\n",
            "|9.0           |[challenge, present, girl, constitute, dying]                                                                      |\n",
            "|10.0          |[brooke, carapace, without, makeup, actress, abuse, barefaced, photograph]                                         |\n",
            "|9.0           |[allhallows, eve, contract, bridge, thomas, acceptable, working, condition]                                        |\n",
            "|10.0          |[ashlee, wallis, warfield, windsor, backless, dress, brings, straw, man, ballet, clique, photo]                    |\n",
            "|10.0          |[fashion, flashback, alexander, wang, ha, come, vitamin, long, way, 6, year, photo]                                |\n",
            "|11.0          |[hidden, mickey, spotting, mick, pas, around, walter, elia, disney, world, deception, kingdom, epcot, photo]       |\n",
            "|9.0           |[trooper, help, deliver, infant, rend, speed, couple]                                                              |\n",
            "|11.0          |[chef, architect, dish, fix, thaiing, castle, downtown, dubai]                                                     |\n",
            "|9.0           |[pricey, mom, prenatal, syndrome, diagnosis]                                                                       |\n",
            "|10.0          |[toothpaste, dry, pimple, top, derms, clear, home, remedy]                                                         |\n",
            "|11.0          |[work, home, rabbi, daughter, finally, shuffling, israel, photo]                                                   |\n",
            "+--------------+-------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Create a new DataFrame with description_filtered as arrays\n",
        "df= df.withColumn('description_filtered', split(col('description_filtered'), ' '))\n",
        "# Show the new DataFrame\n",
        "df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RmBazOpQqXF"
      },
      "source": [
        "## IV- Feature Engineering\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjsTG0YLSyEn"
      },
      "source": [
        "### 1. Explode the filtered descriptions to get the words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qON2vCKBByX",
        "outputId": "3b8af783-1259-46dd-9346-ee5ee60033c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+\n",
            "|     col|\n",
            "+--------+\n",
            "|    best|\n",
            "|    baby|\n",
            "| toddler|\n",
            "| product|\n",
            "|    year|\n",
            "|  summer|\n",
            "|  luxury|\n",
            "|   italy|\n",
            "|    crop|\n",
            "|     top|\n",
            "|   might|\n",
            "|actually|\n",
            "|  parent|\n",
            "| reality|\n",
            "|   leave|\n",
            "|    tyke|\n",
            "|    best|\n",
            "| redness|\n",
            "|   pasta|\n",
            "|   sauce|\n",
            "+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "exploded_df=df.select(explode(df.description_filtered)).alias('words')\n",
        "exploded_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DH41jOsrBByX"
      },
      "outputs": [],
      "source": [
        "#df=df.unpersist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MohTRQirTXD3"
      },
      "source": [
        "### 2. Get unique words in the filtered_description"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NbJFyQlBByX"
      },
      "outputs": [],
      "source": [
        "unique_words=exploded_df.distinct()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-UjxthKU0Rq"
      },
      "source": [
        "### 3. Cache and show the unique words dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1S8h39xBByX",
        "outputId": "8c884387-1347-4441-d313-56027086df18"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 19:====================================================> (194 + 6) / 200]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+\n",
            "|         col|\n",
            "+------------+\n",
            "|     melodic|\n",
            "|   traveling|\n",
            "|         art|\n",
            "|       oscar|\n",
            "|      outfit|\n",
            "|      travel|\n",
            "|       mammy|\n",
            "|       inner|\n",
            "|        pant|\n",
            "|      online|\n",
            "|       still|\n",
            "|     jewelry|\n",
            "|accumulation|\n",
            "|        hope|\n",
            "|      bazaar|\n",
            "|      voyage|\n",
            "|    everyday|\n",
            "|     blossom|\n",
            "|     embrace|\n",
            "|        clog|\n",
            "+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "unique_words=unique_words.cache()\n",
        "unique_words.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCb1pLl3VLOL"
      },
      "source": [
        "### 4. Get the vocabulary size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCakXZ3PBByX",
        "outputId": "e54847cd-6dce-4cf7-ffcb-7770061fcf18"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "128622"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocabulary_size=unique_words.count()\n",
        "vocabulary_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cV9aikKbYQzj"
      },
      "source": [
        "### 5. Define the CountVectorizer and IDF stages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYJp-K7O8pWN"
      },
      "outputs": [],
      "source": [
        "# Define the HashingTF and IDF stages\n",
        "vectorizer = CountVectorizer(inputCol=\"description_filtered\", outputCol=\"raw_features\",vocabSize=vocabulary_size, minDF=3.0)\n",
        "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74T1zTMRxKP8"
      },
      "source": [
        "## V- Models set up, training and evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmY4gqj5Q2w_"
      },
      "source": [
        "### 1. Set up LDA model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8ya3gJ_PxLc",
        "outputId": "63ef37ef-87fb-44e4-88cf-f0c9b94a1bd7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LDA_e18db5c90121"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#num_topics = 20\n",
        "#lda = LDA(k=num_topics, maxIter=10)\n",
        "lda = LDA(featuresCol=\"features\",seed=0)\n",
        "lda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jU2PeRFTlbo9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XT81OlVr5Ck1"
      },
      "source": [
        "### 2. Set up pipelines\n",
        "\n",
        "We will  set up the pipelines of the following transformations for Naive Bayes and Linear reggression\n",
        "\n",
        "- CountVectorizer\n",
        "- IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCKhelqdlbo-",
        "outputId": "52721549-9b15-4352-fec9-26baccf0ee04"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Pipeline_369afe3f2f73"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import CountVectorizer, IDF\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "from pyspark.ml.clustering import LDA\n",
        "\n",
        "# Create pipeline for LDA\n",
        "pipeline = Pipeline(stages=[vectorizer, idf, lda])\n",
        "\n",
        "\n",
        "pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UD0gwMNf7yY7"
      },
      "source": [
        "### 3. Split the data\n",
        "\n",
        "First of all let us split the data into train and test set: 80% for train and 20% for test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pThnAKRy8LqP"
      },
      "outputs": [],
      "source": [
        "# Split data\n",
        "(train_set, test_set) = df.randomSplit([0.8, 0.2], seed=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M84xeE8i67YF"
      },
      "source": [
        "### 4. Create a function for model training\n",
        "\n",
        "Let us create a function which takes as argument a model that it trains and then returns the trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvRF-E1-7ncY"
      },
      "outputs": [],
      "source": [
        "def train_model(model):\n",
        "    return model.fit(train_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9fdT3-7lbpG",
        "outputId": "ebe577b4-4bfc-42bf-aa3b-e125650b7d7f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/06/06 13:51:49 WARN DAGScheduler: Broadcasting large task binary with size 1984.0 KiB\n",
            "24/06/06 13:51:51 WARN DAGScheduler: Broadcasting large task binary with size 1984.0 KiB\n",
            "24/06/06 13:51:52 WARN DAGScheduler: Broadcasting large task binary with size 2000.9 KiB\n",
            "24/06/06 13:51:53 WARN DAGScheduler: Broadcasting large task binary with size 2004.0 KiB\n",
            "24/06/06 13:51:57 WARN DAGScheduler: Broadcasting large task binary with size 2005.1 KiB\n",
            "24/06/06 13:51:58 WARN DAGScheduler: Broadcasting large task binary with size 2000.9 KiB\n",
            "24/06/06 13:51:59 WARN DAGScheduler: Broadcasting large task binary with size 2004.0 KiB\n",
            "24/06/06 13:52:01 WARN DAGScheduler: Broadcasting large task binary with size 2005.1 KiB\n",
            "24/06/06 13:52:02 WARN DAGScheduler: Broadcasting large task binary with size 2000.9 KiB\n",
            "24/06/06 13:52:02 WARN DAGScheduler: Broadcasting large task binary with size 2004.0 KiB\n",
            "24/06/06 13:52:04 WARN DAGScheduler: Broadcasting large task binary with size 2005.1 KiB\n",
            "24/06/06 13:52:07 WARN DAGScheduler: Broadcasting large task binary with size 2000.9 KiB\n",
            "24/06/06 13:52:08 WARN DAGScheduler: Broadcasting large task binary with size 2004.0 KiB\n",
            "24/06/06 13:52:10 WARN DAGScheduler: Broadcasting large task binary with size 2005.1 KiB\n",
            "24/06/06 13:52:10 WARN DAGScheduler: Broadcasting large task binary with size 2000.9 KiB\n",
            "24/06/06 13:52:11 WARN DAGScheduler: Broadcasting large task binary with size 2004.0 KiB\n",
            "24/06/06 13:52:13 WARN DAGScheduler: Broadcasting large task binary with size 2005.1 KiB\n",
            "24/06/06 13:52:14 WARN DAGScheduler: Broadcasting large task binary with size 2000.9 KiB\n",
            "24/06/06 13:52:14 WARN DAGScheduler: Broadcasting large task binary with size 2004.0 KiB\n",
            "24/06/06 13:52:16 WARN DAGScheduler: Broadcasting large task binary with size 2005.1 KiB\n",
            "24/06/06 13:52:17 WARN DAGScheduler: Broadcasting large task binary with size 2000.9 KiB\n",
            "24/06/06 13:52:17 WARN DAGScheduler: Broadcasting large task binary with size 2004.0 KiB\n",
            "24/06/06 13:52:19 WARN DAGScheduler: Broadcasting large task binary with size 2005.1 KiB\n",
            "24/06/06 13:52:20 WARN DAGScheduler: Broadcasting large task binary with size 2000.9 KiB\n",
            "24/06/06 13:52:20 WARN DAGScheduler: Broadcasting large task binary with size 2004.0 KiB\n",
            "24/06/06 13:52:22 WARN DAGScheduler: Broadcasting large task binary with size 2005.1 KiB\n",
            "24/06/06 13:52:22 WARN DAGScheduler: Broadcasting large task binary with size 2000.9 KiB\n",
            "24/06/06 13:52:23 WARN DAGScheduler: Broadcasting large task binary with size 2004.0 KiB\n",
            "24/06/06 13:52:24 WARN DAGScheduler: Broadcasting large task binary with size 2005.1 KiB\n",
            "24/06/06 13:52:25 WARN DAGScheduler: Broadcasting large task binary with size 2000.9 KiB\n",
            "24/06/06 13:52:26 WARN DAGScheduler: Broadcasting large task binary with size 2004.0 KiB\n",
            "24/06/06 13:52:27 WARN DAGScheduler: Broadcasting large task binary with size 2005.1 KiB\n",
            "24/06/06 13:52:28 WARN DAGScheduler: Broadcasting large task binary with size 2000.9 KiB\n",
            "24/06/06 13:52:28 WARN DAGScheduler: Broadcasting large task binary with size 2004.0 KiB\n",
            "24/06/06 13:52:30 WARN DAGScheduler: Broadcasting large task binary with size 2005.1 KiB\n",
            "24/06/06 13:52:31 WARN DAGScheduler: Broadcasting large task binary with size 2000.9 KiB\n",
            "24/06/06 13:52:31 WARN DAGScheduler: Broadcasting large task binary with size 2004.0 KiB\n",
            "24/06/06 13:52:33 WARN DAGScheduler: Broadcasting large task binary with size 2005.1 KiB\n",
            "24/06/06 13:52:34 WARN DAGScheduler: Broadcasting large task binary with size 2000.9 KiB\n",
            "24/06/06 13:52:34 WARN DAGScheduler: Broadcasting large task binary with size 2004.0 KiB\n",
            "24/06/06 13:52:35 WARN DAGScheduler: Broadcasting large task binary with size 2005.1 KiB\n",
            "24/06/06 13:52:36 WARN DAGScheduler: Broadcasting large task binary with size 2000.9 KiB\n",
            "24/06/06 13:52:37 WARN DAGScheduler: Broadcasting large task binary with size 2004.0 KiB\n",
            "24/06/06 13:52:38 WARN DAGScheduler: Broadcasting large task binary with size 2005.1 KiB\n",
            "24/06/06 13:52:39 WARN DAGScheduler: Broadcasting large task binary with size 2000.9 KiB\n",
            "24/06/06 13:52:39 WARN DAGScheduler: Broadcasting large task binary with size 2004.0 KiB\n",
            "24/06/06 13:52:41 WARN DAGScheduler: Broadcasting large task binary with size 2005.1 KiB\n",
            "24/06/06 13:52:42 WARN DAGScheduler: Broadcasting large task binary with size 2000.9 KiB\n",
            "24/06/06 13:52:42 WARN DAGScheduler: Broadcasting large task binary with size 2004.0 KiB\n",
            "24/06/06 13:52:43 WARN DAGScheduler: Broadcasting large task binary with size 2005.1 KiB\n",
            "24/06/06 13:52:44 WARN DAGScheduler: Broadcasting large task binary with size 2000.9 KiB\n",
            "24/06/06 13:52:45 WARN DAGScheduler: Broadcasting large task binary with size 2004.0 KiB\n",
            "24/06/06 13:52:46 WARN DAGScheduler: Broadcasting large task binary with size 2005.1 KiB\n",
            "24/06/06 13:52:47 WARN DAGScheduler: Broadcasting large task binary with size 2000.9 KiB\n",
            "24/06/06 13:52:47 WARN DAGScheduler: Broadcasting large task binary with size 2004.0 KiB\n",
            "24/06/06 13:52:49 WARN DAGScheduler: Broadcasting large task binary with size 2005.1 KiB\n",
            "24/06/06 13:52:49 WARN DAGScheduler: Broadcasting large task binary with size 2000.9 KiB\n",
            "24/06/06 13:52:50 WARN DAGScheduler: Broadcasting large task binary with size 2004.0 KiB\n",
            "24/06/06 13:52:51 WARN DAGScheduler: Broadcasting large task binary with size 2005.1 KiB\n",
            "24/06/06 13:52:52 WARN DAGScheduler: Broadcasting large task binary with size 2000.9 KiB\n",
            "24/06/06 13:52:53 WARN DAGScheduler: Broadcasting large task binary with size 2004.0 KiB\n",
            "24/06/06 13:52:54 WARN DAGScheduler: Broadcasting large task binary with size 2005.1 KiB\n",
            "                                                                                \r"
          ]
        },
        {
          "data": {
            "text/plain": [
              "PipelineModel_01bccd6396f8"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fitted_model=train_model(pipeline)\n",
        "fitted_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6EEx9rvlbpG"
      },
      "source": [
        "### 5. Visualize the topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E265riGdlbpG",
        "outputId": "dcf11618-c0a5-46df-9e66-3c0f1e79c48f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "73411"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fitted_vectirizer=fitted_model.stages[0]\n",
        "vocabulary= fitted_vectirizer.vocabulary\n",
        "len(vocabulary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6TGqC1_3lbpG",
        "outputId": "3de23766-c402-4b85-f5b9-2403e0a83d25"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['new', 'photo', 'state', 'trump', 'day', 'nt', 'say', 'woman', 'get', 'make']"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocabulary[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9Jq8bDTlbpH",
        "outputId": "c43e0af3-fec8-48ad-f9d1-b27e27d84f38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+--------------------+--------------------+\n",
            "|topic|         termIndices|         termWeights|\n",
            "+-----+--------------------+--------------------+\n",
            "|    0|[7, 137, 2, 318, ...|[0.00404272871093...|\n",
            "|    1|[3, 36, 107, 4, 1...|[0.00444673252088...|\n",
            "|    2|[30, 16, 0, 57, 1...|[0.00510976670347...|\n",
            "|    3|[4, 1, 11, 26, 34...|[0.00456010441787...|\n",
            "|    4|[27, 3, 0, 2, 47,...|[0.00347962927032...|\n",
            "|    5|[35, 0, 187, 5, 1...|[0.00399826687904...|\n",
            "|    6|[100, 0, 123, 1, ...|[0.00358280357609...|\n",
            "|    7|[230, 328, 222, 2...|[0.00373880100877...|\n",
            "|    8|[0, 8, 1, 22, 56,...|[0.00302276662198...|\n",
            "|    9|[21, 93, 5, 26, 5...|[0.00814480394485...|\n",
            "+-----+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFOTtJHmlbpH",
        "outputId": "7b1a9d7d-3303-4dd1-f121-1651e27bad7a"
      },
      "outputs": [
        {
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o252.javaToPython.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:834)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\n\tat org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2707)\n\tat org.apache.spark.sql.SparkSession.$anonfun$leafNodeDefaultParallelism$1(SparkSession.scala:906)\n\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.SparkSession.leafNodeDefaultParallelism(SparkSession.scala:906)\n\tat org.apache.spark.sql.execution.LocalTableScanExec.rdd$lzycompute(LocalTableScanExec.scala:53)\n\tat org.apache.spark.sql.execution.LocalTableScanExec.rdd(LocalTableScanExec.scala:48)\n\tat org.apache.spark.sql.execution.LocalTableScanExec.doExecute(LocalTableScanExec.scala:60)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:207)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:206)\n\tat org.apache.spark.sql.Dataset.javaToPython(Dataset.scala:4139)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "Input \u001b[0;32mIn [33]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m topics_rdd \u001b[38;5;241m=\u001b[39m \u001b[43mtopics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\n\u001b[1;32m      2\u001b[0m topics_words \u001b[38;5;241m=\u001b[39m topics_rdd\\\n\u001b[1;32m      3\u001b[0m        \u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m row: row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtermIndices\u001b[39m\u001b[38;5;124m'\u001b[39m])\\\n\u001b[1;32m      4\u001b[0m        \u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m idx_list: [vocabulary[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m idx_list])\\\n\u001b[1;32m      5\u001b[0m        \u001b[38;5;241m.\u001b[39mcollect()\n\u001b[1;32m      6\u001b[0m topics_words[:\u001b[38;5;241m2\u001b[39m]\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/dataframe.py:214\u001b[0m, in \u001b[0;36mDataFrame.rdd\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;124m\"\"\"Returns the content as an :class:`pyspark.RDD` of :class:`Row`.\u001b[39m\n\u001b[1;32m    200\u001b[0m \n\u001b[1;32m    201\u001b[0m \u001b[38;5;124m.. versionadded:: 1.3.0\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;124m<class 'pyspark.rdd.RDD'>\u001b[39m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lazy_rdd \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 214\u001b[0m     jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjavaToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lazy_rdd \u001b[38;5;241m=\u001b[39m RDD(\n\u001b[1;32m    216\u001b[0m         jrdd, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession\u001b[38;5;241m.\u001b[39m_sc, BatchedSerializer(CPickleSerializer())\n\u001b[1;32m    217\u001b[0m     )\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lazy_rdd\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o252.javaToPython.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:834)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\n\tat org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2707)\n\tat org.apache.spark.sql.SparkSession.$anonfun$leafNodeDefaultParallelism$1(SparkSession.scala:906)\n\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.SparkSession.leafNodeDefaultParallelism(SparkSession.scala:906)\n\tat org.apache.spark.sql.execution.LocalTableScanExec.rdd$lzycompute(LocalTableScanExec.scala:53)\n\tat org.apache.spark.sql.execution.LocalTableScanExec.rdd(LocalTableScanExec.scala:48)\n\tat org.apache.spark.sql.execution.LocalTableScanExec.doExecute(LocalTableScanExec.scala:60)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:207)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:206)\n\tat org.apache.spark.sql.Dataset.javaToPython(Dataset.scala:4139)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jPE8W6AlbpH"
      },
      "outputs": [],
      "source": [
        "topics = fitted_model.stages[-1].describeTopics()\n",
        "topics.show()\n",
        "\n",
        "topics_rdd = topics.rdd\n",
        "topics_words = topics_rdd\\\n",
        "       .map(lambda row: row['termIndices'])\\\n",
        "       .map(lambda idx_list: [vocabulary[idx] for idx in idx_list])\\\n",
        "       .collect()\n",
        "topics_words[:2]\n",
        "\n",
        "\n",
        "\n",
        "for idx, topic in enumerate(topics_words):\n",
        "    print(\"topic: {}\".format(idx))\n",
        "    print(\"*\"*25)\n",
        "    for word in topic:\n",
        "       print(word)\n",
        "    print(\"*\"*25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFhj3jcDlbpH"
      },
      "source": [
        "### Get topics distributions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxDtRTLllbpI"
      },
      "outputs": [],
      "source": [
        "# Transform the training and test data\n",
        "train_set_transformed = fitted_model.transform(train_set)\n",
        "test_set_transformed = fitted_model.transform(test_set)\n",
        "\n",
        "# Get the LDA model from the pipeline model\n",
        "lda_model = fitted_model.stages[-1]\n",
        "\n",
        "# Extract the topic distributions\n",
        "train_topic_distributions = train_set_transformed.select(\"description_filtered\", \"topicDistribution\")\n",
        "test_topic_distributions = test_set_transformed.select(\"description_filtered\", \"topicDistribution\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thwp6950lbpI"
      },
      "outputs": [],
      "source": [
        "# Show the topic distributions for the training set\n",
        "train_topic_distributions.show(truncate=False)\n",
        "\n",
        "# Show the topic distributions for the test set\n",
        "test_topic_distributions.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8_hqauylbpI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-fBEB1f9aRD"
      },
      "source": [
        "### 5. Define a function to evaluate the model\n",
        "\n",
        "The function takes as parameter a fitted model, evaluates the model on train and test split and then return the train and test performance. The accuracy is the metric used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kDQDHNqlbpI"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ouwyZ0YFlbpJ",
        "outputId": "4ed53310-2704-4512-dde3-68b61e247dd5"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'train_set_transformed' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Input \u001b[0;32mIn [29]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66m# Function to evaluate model and get best parameters\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_model\u001b[39m(fitted_model,data_transformed\u001b[38;5;241m=\u001b[39m[\u001b[43mtrain_set_transformed\u001b[49m,test_set_transformed]):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvaluating the model on training set\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m     train_lp \u001b[38;5;241m=\u001b[39m fitted_model\u001b[38;5;241m.\u001b[39mlogPerplexity(data_transformed[\u001b[38;5;241m0\u001b[39m])\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_set_transformed' is not defined"
          ]
        }
      ],
      "source": [
        "# Function to evaluate model and get best parameters\n",
        "def evaluate_model(fitted_model,data_transformed=[train_set_transformed,test_set_transformed]):\n",
        "\n",
        "    print('Evaluating the model on training set')\n",
        "    train_lp = fitted_model.logPerplexity(data_transformed[0])\n",
        "\n",
        "    print('Evaluating the model on test set')\n",
        "    test_lp = fitted_model.logPerplexity(data_transformed[1])\n",
        "\n",
        "    print(\"The upper bound on perplexity for train set: \" + str(train_lp))\n",
        "    print(\"The upper bound on perplexity for test set: \" + str(test_lp))\n",
        "    return train_lp, test_lp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYoDfhgplbpJ"
      },
      "outputs": [],
      "source": [
        "train_lp,test_lp=evaluate_model(fitted_model.stages[-1])\n",
        "train_lp,test_lp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HnyIPBvFlbpJ"
      },
      "outputs": [],
      "source": [
        "def"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ie8iCBP0lbpJ"
      },
      "outputs": [],
      "source": [
        "results={}\n",
        "num_topics_range=[20, 25, 30, 35, 40, 45, 50]\n",
        "\n",
        "for num_topics in num_topics_range:\n",
        "    print('LDA for k={}'.format(num_topics))\n",
        "    # Create LDA\n",
        "    lda = LDA(featuresCol=\"features\",seed=0)\n",
        "    # Create pipeline for LDA\n",
        "    pipeline = Pipeline(stages=[vectorizer, idf, lda])\n",
        "    print('Model training')\n",
        "    # Train the model\n",
        "    fitted_model=train_model(pipeline)\n",
        "    print('Done')\n",
        "\n",
        "    train_set_transformed = fitted_model.transform(train_set)\n",
        "    test_set_transformed = fitted_model.transform(test_set)\n",
        "    train_lp,test_lp=evaluate_model(fitted_model,data_transformed=[train_set_transformed,test_set_transformed])\n",
        "    results[num_topics]=\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yG6dMBjzLXWA"
      },
      "source": [
        "### 6. Create a function which takes pipelines and train the models, evaluate them and then return the results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "808-IeqCNb1v"
      },
      "source": [
        "We remark that\n",
        "- Naive Bayes\n",
        "- Logistic regression\n",
        "\n",
        "We can then conclude that t\n",
        "- he two models set a good performance on both training and test set.\n",
        "- The Logistic regression models outperforms the Naive Bayes model\n",
        "\n",
        "In the next section, we will tune the parameters of the Naive bayes to get the best parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkME6ByNOegG"
      },
      "source": [
        "## VI- Logistic regression hyperparameters tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAWi0NqwSzgp"
      },
      "source": [
        "### 1. Pipeline creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wtq5K0JbOdpD"
      },
      "outputs": [],
      "source": [
        "# Define parameter grids for Logistic regresion grid search\n",
        "reg_values = np.logspace(-4, 4, num=100)\n",
        "l1_ratios = np.linspace(0, 1, num=10)\n",
        "\n",
        "paramGrid_lr=paramGrid_lr.addGrid(lr.regParam, reg_values).build()\n",
        "\n",
        "# Create Cross-validation for Logistic Regression\n",
        "cv_lr = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid_lr,\n",
        "                        evaluator=MulticlassClassificationEvaluator(labelCol=\"category_label\", predictionCol=\"prediction\", metricName=\"accuracy\"),\n",
        "                        numFolds=3, parallelism=1)\n",
        "\n",
        "\n",
        "# Create pipeline for Logistic Regression\n",
        "pipeline_lr = Pipeline(stages=[hashingTF, idf, cv_lr])\n",
        "\n",
        "pipeline_lr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XhMJxEzS87h"
      },
      "source": [
        "### 2. Hyperparameters tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bSLe3K5TL1L"
      },
      "outputs": [],
      "source": [
        "results=train_and_evaluate_models(model_pipelines=[pipeline_lr],model_names=[\"Logistic Regression\"])\n",
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUlLvsWVTkDy"
      },
      "source": [
        "### 3. Get the best parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5rCb_boTk3e"
      },
      "outputs": [],
      "source": [
        "fitted_model=results['fitted_model']\n",
        "\n",
        "# Get the best model\n",
        "best_model = fitted_model.stages[-1].bestModel\n",
        "\n",
        "# Print the best parameters\n",
        "print(f\"Best parameters for Logistic regression:\")\n",
        "\n",
        "for param, value in best_model.extractParamMap().items():\n",
        "     print(f\"  {param.name}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEH0OfIqUxOg"
      },
      "source": [
        "### 4. Save the best model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xX4pmvAVBByZ",
        "outputId": "843aa553-e419-43ba-dbb3-3ebb1a67f454"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/06/04 20:03:43 WARN TaskSetManager: Stage 216 contains a task of very large size (33450 KiB). The maximum recommended task size is 1000 KiB.\n",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "best_model.save('output/news_categorization_model')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnFsw8x5Vyy2"
      },
      "source": [
        "## VII- Summary\n",
        "\n",
        "In this notebook we have studied two models for our news categorization task. There are Naive Bayes and Logistic regression.\n",
        "\n",
        " Our study reveals that the Logistic regression was the one with best performance.\n",
        "\n",
        " Then we tunned the Logistic regression hyperparameters using grid search and then we find the best model that we save.\n",
        "\n",
        " The next step of our work will be to ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSyD5awwBByb"
      },
      "outputs": [],
      "source": [
        "#df.unpersist()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}