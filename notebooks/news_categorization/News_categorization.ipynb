{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guYKqw-LBByQ",
        "outputId": "0cb914b5-c60e-4d61-8c9b-7808a57effca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=ce524e6b261d3c4f86eec1ca1cc19106341a686bccf8b498ee4e05bdc4dc2cad\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-qFVy2lBaqu"
      },
      "source": [
        "# News categorization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## I- Modules import"
      ],
      "metadata": {
        "id": "mNfMRWDbFXDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.ml.feature import  IDF, HashingTF\n",
        "from pyspark.ml import  Pipeline\n",
        "from math import ceil,log2\n",
        "from pyspark.ml.classification import LogisticRegression,NaiveBayes,LogisticRegressionModel\n",
        "from pyspark.sql.functions import col,explode,split\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "3QYNQcr-FQLs"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## II- Spark context and session creation"
      ],
      "metadata": {
        "id": "Nj7Qu-EvK9_Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "aFZWn6-QBByS",
        "outputId": "b7430067-1724-411c-c9fb-aa0e1cf8140e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://node02.cm.cluster:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>spark://node02:7077</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>NewsCategorisation</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fff8e1c6250>"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spark = (SparkSession.builder\n",
        "    .master(\"spark://node02:7077\")\n",
        "    .appName(\"NewsCategorisation\")\n",
        "    #.config('spark.driver.cores','4')\n",
        "    #.config(\"spark.executorEnv.LD_LIBRARY_PATH\", \"/home/team1337/.local/easybuild_new/software/Python/3.8.6-GCCcore-10.2.0/lib:/home/team1337/.local/easybuild_new/software/libffi/3.3-GCCcore-10.2.0/lib64:/home/team1337/.local/easybuild_new/software/GMP/6.2.0-GCCcore-10.2.0/lib:/home/team1337/.local/easybuild_new/software/XZ/5.2.5-GCCcore-10.2.0/lib:/home/team1337/.local/easybuild_new/software/SQLite/3.33.0-GCCcore-10.2.0/lib:/home/team1337/.local/easybuild_new/software/Tcl/8.6.10-GCCcore-10.2.0/lib:/home/team1337/.local/easybuild_new/software/libreadline/8.0-GCCcore-10.2.0/lib:/home/team1337/.local/easybuild_new/software/ncurses/6.2-GCCcore-10.2.0/lib:/home/team1337/.local/easybuild_new/software/bzip2/1.0.8-GCCcore-10.2.0/lib:/home/team1337/.local/easybuild_new/software/binutils/2.35-GCCcore-10.2.0/lib:/home/team1337/.local/easybuild_new/software/zlib/1.2.11-GCCcore-10.2.0/lib:/home/team1337/.local/easybuild_new/software/GCCcore/10.2.0/lib64\") \\\n",
        "    #.config(\"spark.pyspark.python\", \"/home/team1337/.local/easybuild_new/software/Python/3.8.6-GCCcore-10.2.0/bin/python3\") \\\n",
        "    .getOrCreate()\n",
        "        )\n",
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## III- Dataframe preparation"
      ],
      "metadata": {
        "id": "PuV9CWoMN60A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Load the data"
      ],
      "metadata": {
        "id": "Em4ZWvqhPoHt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xMI_45McvZto",
        "outputId": "7b2ef733-bdeb-4756-b088-efe035e50be6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r",
            "[Stage 0:>                                                          (0 + 1) / 1]\r",
            "\r",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Load data\n",
        "df = spark.read.parquet(\"input/news.parquet\", header=True, inferSchema=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Partition and cache the dataframe"
      ],
      "metadata": {
        "id": "x4csdl2FOVf8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4EXrJ3GIBByT",
        "outputId": "ddd03fd8-a9c6-436c-abe7-a1828a821abe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.rdd.getNumPartitions()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPBefKKqBByU"
      },
      "outputs": [],
      "source": [
        "num_partitions=5*40\n",
        "df= df.repartition(num_partitions).cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ytFY1tjRBByU",
        "outputId": "fce9d3ee-051f-4b09-b5dd-c8a834dc3e22"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 3:============================================>         (165 + 22) / 200]\r"
          ]
        },
        {
          "data": {
            "text/plain": [
              "200"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.rdd.getNumPartitions()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Preview the data"
      ],
      "metadata": {
        "id": "4G5XozIiQTEx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r86OcjwLv0EU",
        "outputId": "908b16c6-118c-4870-fef3-2dc4086a5ab1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r",
            "                                                                                \r"
          ]
        },
        {
          "data": {
            "text/plain": [
              "1716608"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfy_ivN4BByV",
        "outputId": "047c2814-4f8d-4015-b1bd-ab3e4a3758c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------+--------------------+\n",
            "|category_label|description_filtered|\n",
            "+--------------+--------------------+\n",
            "|          10.0|every bozo need k...|\n",
            "|          11.0|best redness past...|\n",
            "|          10.0|fashion show used...|\n",
            "|           9.0|challenge present...|\n",
            "|          10.0|man vintage show ...|\n",
            "|          11.0|hidden mickey spo...|\n",
            "|          10.0|next fashion uppe...|\n",
            "|          11.0|work home rabbi d...|\n",
            "|           9.0|mommy manage nt g...|\n",
            "|           9.0|ontogenesis hormo...|\n",
            "|          10.0|7 gross grooming ...|\n",
            "|           9.0|period display bo...|\n",
            "|          10.0|transformation ph...|\n",
            "|          10.0|morena baccarin g...|\n",
            "|          11.0|expat recovery ro...|\n",
            "|          11.0|amsterdam diverse...|\n",
            "|          11.0|view afar make ma...|\n",
            "|          10.0|new house york fa...|\n",
            "|           9.0|5 dumbest affair ...|\n",
            "|           9.0|   valentine day kid|\n",
            "+--------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "id": "awWri7zZQcOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Convert filtered descriptions to arrays"
      ],
      "metadata": {
        "id": "VMpXsNNsO_Z6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Drqnhee0i26r",
        "outputId": "ca8e510c-4855-440f-d3e1-a39f6b13a50d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------+----------------------------------------------------------------------------------------------------------------+\n",
            "|category_label|description_filtered                                                                                            |\n",
            "+--------------+----------------------------------------------------------------------------------------------------------------+\n",
            "|10.0          |[every, bozo, need, know, fashion]                                                                              |\n",
            "|11.0          |[best, redness, pasta, sauce, italian, love, life, ve, never, heard]                                            |\n",
            "|10.0          |[fashion, show, used, represent, raucous, liaison, video]                                                       |\n",
            "|9.0           |[challenge, present, girl, constitute, dying]                                                                   |\n",
            "|10.0          |[man, vintage, show, delivers, carefully, curated, habiliment, culled, around, land]                            |\n",
            "|11.0          |[hidden, mickey, spotting, mick, pas, around, walter, elia, disney, world, deception, kingdom, epcot, photo]    |\n",
            "|10.0          |[next, fashion, uppercase, letter, south]                                                                       |\n",
            "|11.0          |[work, home, rabbi, daughter, finally, shuffling, israel, photo]                                                |\n",
            "|9.0           |[mommy, manage, nt, get, want, nursemaid]                                                                       |\n",
            "|9.0           |[ontogenesis, hormone, injectant, subject, logo]                                                                |\n",
            "|10.0          |[7, gross, grooming, mistake, men, make, work]                                                                  |\n",
            "|9.0           |[period, display, board, game, design, destigmatize, menstruation]                                              |\n",
            "|10.0          |[transformation, photo, prove, photoshop, isnt, thing, making, model, look, perfect]                            |\n",
            "|10.0          |[morena, baccarin, golden, globe, dress, 2013, see, red, carpet, aspect, photograph]                            |\n",
            "|11.0          |[expat, recovery, room, bear, blow, travel]                                                                     |\n",
            "|11.0          |[amsterdam, diverse, night, life]                                                                               |\n",
            "|11.0          |[view, afar, make, mardi, gras, new, siege, orleans]                                                            |\n",
            "|10.0          |[new, house, york, fashion, week, front, man, row, famous, person, include, kirsten, dunst, minka, kelly, photo]|\n",
            "|9.0           |[5, dumbest, affair, first, two, year, beginner]                                                                |\n",
            "|9.0           |[valentine, day, kid]                                                                                           |\n",
            "+--------------+----------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Create a new DataFrame with description_filtered as arrays\n",
        "df= df.withColumn('description_filtered', split(col('description_filtered'), ' '))\n",
        "# Show the new DataFrame\n",
        "df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IV- Feature Engineering\n"
      ],
      "metadata": {
        "id": "3RmBazOpQqXF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Explode the filtered descriptions to get the words"
      ],
      "metadata": {
        "id": "LjsTG0YLSyEn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qON2vCKBByX"
      },
      "outputs": [],
      "source": [
        "exploded_df=df.select(explode(df.description_filtered)).alias('words')\n",
        "exploded_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DH41jOsrBByX"
      },
      "outputs": [],
      "source": [
        "#df=df.unpersist()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Get unique words in the filtered_description"
      ],
      "metadata": {
        "id": "MohTRQirTXD3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NbJFyQlBByX"
      },
      "outputs": [],
      "source": [
        "unique_words=exploded_df.distinct()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Cache and show the unique words dataframe"
      ],
      "metadata": {
        "id": "t-UjxthKU0Rq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1S8h39xBByX"
      },
      "outputs": [],
      "source": [
        "unique_words=unique_words.cache()\n",
        "unique_words.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Get the vocabulary size"
      ],
      "metadata": {
        "id": "KCb1pLl3VLOL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCakXZ3PBByX",
        "outputId": "c391ee19-f5c6-4a9e-cc53-4c4cd0b031e1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "128622"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocabulary_size=unique_words.count()\n",
        "vocabulary_size"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Unpersit the unique words dataframe(not needed anymore)"
      ],
      "metadata": {
        "id": "ycUp4mQDVbZ1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nS-8eBNnBByX"
      },
      "outputs": [],
      "source": [
        "unique_words=unique_words.unpersist()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Get the smallest `n` such that $2^n$ is greater than `vocabulary_size`"
      ],
      "metadata": {
        "id": "irYncNP3WWhr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_features=log2(vocabulary_size)\n"
      ],
      "metadata": {
        "id": "9kdafIt4WnP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0nzNFsoBByY",
        "outputId": "83b3aebf-db97-48f0-df3e-46aa7c20ea0b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "17"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "n=ceil(log2(vocabulary_size))\n",
        "n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Get the number of features for HashingTF"
      ],
      "metadata": {
        "id": "mtwfZn61Yzh-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_features=2**n\n",
        "num_features"
      ],
      "metadata": {
        "id": "FviyZsE_YxxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Define the HashingTF and IDF stages"
      ],
      "metadata": {
        "id": "cV9aikKbYQzj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the HashingTF and IDF stages\n",
        "hashingTF = HashingTF(inputCol=\"description_filtered\", outputCol=\"rawFeatures\", numFeatures=num_features)\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")"
      ],
      "metadata": {
        "id": "bIL-PyIHYm4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## V- Models set up, training and evaluation"
      ],
      "metadata": {
        "id": "74T1zTMRxKP8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Create a function to set up pipelines\n",
        "\n",
        "We will create a function which set up and returns  the pipelines of the following transformations for Native Bayes and Linear reggression\n",
        "\n",
        "- HashingTF\n",
        "- IDF\n",
        "- 3-Fold Cross-validation  with eventually grid search\n",
        "\n",
        "Our function takes as optional argument a boolean with False as default value, which specifies wether to perform grid search or not."
      ],
      "metadata": {
        "id": "XT81OlVr5Ck1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LArI4unEDer1",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def set_up_pipelines(grid_search=False):\n",
        "\n",
        "    # Define the classifiers\n",
        "\n",
        "    # Logistic regression classifier\n",
        "    lr = LogisticRegression(labelCol=\"category_label\", featuresCol=\"features\")\n",
        "\n",
        "    # Naive Bayes classifier\n",
        "    nb = NaiveBayes(labelCol=\"category_label\", featuresCol=\"features\")\n",
        "\n",
        "    # Define parameter grids\n",
        "    paramGrid_nb = (ParamGridBuilder()\n",
        "        .addGrid(nb.smoothing, [0.5, 1.0, 2.0])\n",
        "        .build())\n",
        "\n",
        "    paramGrid_nb=paramGrid_lr=ParamGridBuilder()\n",
        "\n",
        "    if grid_search:\n",
        "        # Define parameter grids for Native Bayes grid search\n",
        "        reg_values = np.logspace(-4, 4, num=100)\n",
        "        l1_ratios = np.linspace(0, 1, num=10)\n",
        "\n",
        "        # Add  parameters to the grid\n",
        "        paramGrid_nb=paramGrid_nb.addGrid(nb.smoothing, [0.5, 1.0, 2.0])\n",
        "        paramGrid_lr=paramGrid_lr.addGrid(lr.regParam, reg_values)\n",
        "\n",
        "    # Build the parmaeters grids\n",
        "    paramGrid_nb = paramGrid_nb.build()\n",
        "    paramGrid_lr = paramGrid_lr.build()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Create cross validators\n",
        "\n",
        "    # Cross-validation for Naive Bayes\n",
        "    cv_nb = CrossValidator(estimator=nb, estimatorParamMaps=paramGrid_nb,\n",
        "                        evaluator=MulticlassClassificationEvaluator(labelCol=\"category_label\", predictionCol=\"prediction\", metricName=\"accuracy\"),\n",
        "                        numFolds=3, parallelism=1)\n",
        "    # Cross-validation for Logistic Regression\n",
        "    cv_lr = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid_lr,\n",
        "                        evaluator=MulticlassClassificationEvaluator(labelCol=\"category_label\", predictionCol=\"prediction\", metricName=\"accuracy\"),\n",
        "                        numFolds=3, parallelism=1)\n",
        "\n",
        "\n",
        "    # Create pipelines\n",
        "    # Pipeline for Naive Bayes\n",
        "    pipeline_nb = Pipeline(stages=[hashingTF, idf, cv_nb])\n",
        "    # Pipeline for Logistic Regression\n",
        "    pipeline_lr = Pipeline(stages=[hashingTF, idf, cv_lr])\n",
        "\n",
        "    # Return the pipelines\n",
        "    return pipeline_nb, pipeline_lr\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Split the data\n",
        "\n",
        "First of all let us split the data into train and test set: 80% for train and 20% for test"
      ],
      "metadata": {
        "id": "UD0gwMNf7yY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data\n",
        "(train_set, test_set) = df.randomSplit([0.8, 0.2], seed=0)"
      ],
      "metadata": {
        "id": "pThnAKRy8LqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Create a function for model training\n",
        "\n",
        "Let us create a function which takes as argument a model that it trains and then returns the trained model."
      ],
      "metadata": {
        "id": "M84xeE8i67YF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model):\n",
        "    print('Training the model')\n",
        "    fitted_model = model.fit(train_set)\n",
        "    print('Done')\n",
        "    return fitted_model"
      ],
      "metadata": {
        "id": "AvRF-E1-7ncY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Define a function to evaluate the model\n",
        "\n",
        "The function takes as parameter a fitted model, evaluates the model on train and test split and then return the train and test performance. The accuracy is the metric used."
      ],
      "metadata": {
        "id": "K-fBEB1f9aRD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIKVzBw1BByY"
      },
      "outputs": [],
      "source": [
        "# Initialize the evaluator\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"category_label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "\n",
        "# Function to evaluate model and get best parameters\n",
        "def evaluate_model(fitted_model):\n",
        "\n",
        "    print('Making predictions on the training set')\n",
        "\n",
        "    train_predictions = fitted_model.transform(train_set)\n",
        "\n",
        "    print('Making predictions on the test set')\n",
        "    test_predictions = fitted_model.transform(test_set)\n",
        "\n",
        "    print('Evaluating the model on training set')\n",
        "    train_accuracy = evaluator.evaluate(train_predictions)\n",
        "\n",
        "    print('Evaluating the model on test set')\n",
        "    test_accuracy = evaluator.evaluate(test_predictions)\n",
        "    return train_accuracy, test_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to evaluate model and get best parameters\n",
        "def evaluate_model(model, model_name):\n",
        "    print('Training the model')\n",
        "\n",
        "    # Train the model using cross-validation\n",
        "    fitted_model = model.fit(train_set)\n",
        "\n",
        "    # Get the best model from cross-validation\n",
        "    best_model = fitted_model.stages[-1].bestModel\n",
        "\n",
        "    print('Making predictions on the training set')\n",
        "    # Make predictions on the training set\n",
        "    train_predictions = fitted_model.transform(train_set)\n",
        "\n",
        "    print('Making predictions on the test set')\n",
        "    # Make predictions on the test set\n",
        "    test_predictions = fitted_model.transform(test_set)\n",
        "\n",
        "    # Initialize the evaluator\n",
        "    evaluator = MulticlassClassificationEvaluator(labelCol=\"category_label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "\n",
        "    print('Evaluating the model on training set')\n",
        "    # Evaluate the model on the training set\n",
        "    train_accuracy = evaluator.evaluate(train_predictions)\n",
        "\n",
        "    print('Evaluating the model on test set')\n",
        "    # Evaluate the model on the test set\n",
        "    test_accuracy = evaluator.evaluate(test_predictions)\n",
        "\n",
        "    print(f\"{model_name} Train Accuracy: {train_accuracy}\")\n",
        "    print(f\"{model_name} Test Accuracy: {test_accuracy}\")\n",
        "\n",
        "    # Print the best parameters\n",
        "    print(f\"Best parameters for {model_name}:\")\n",
        "\n",
        "    for param, value in best_model.extractParamMap().items():\n",
        "        print(f\"  {param.name}: {value}\")\n",
        "\n",
        "    return train_accuracy, test_accuracy,best_model"
      ],
      "metadata": {
        "id": "eJHhpCCj-jCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Call the functions and interpret the results"
      ],
      "metadata": {
        "id": "pYDcrjUHBQHf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### a Set up the pipelines"
      ],
      "metadata": {
        "id": "-hHm6hxi5Tdj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the pipelins\n",
        "model_pipelines  = set_up_pipelines()"
      ],
      "metadata": {
        "id": "ErNuZLi64UYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Naive Bayes pipeline\n",
        "model_pipelines[0]"
      ],
      "metadata": {
        "id": "IuKHC8ju4gnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic regression pipeline\n",
        "model_pipelines"
      ],
      "metadata": {
        "id": "JyIZClex4uTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### b. Train the two models"
      ],
      "metadata": {
        "id": "5i8kyQBECB0P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = {}\n",
        "\n",
        "model_names = [\"Naive Bayes\", \"Logistic Regression\"]\n",
        "\n",
        "# Initialize the results dictionary\n",
        "results = []\n",
        "\n",
        "# Loop over the indices and model names simultaneously\n",
        "for idx, (model_pipeline, model_name) in enumerate(zip(model_pipelines, model_names)):\n",
        "    results[idx] = {'model_name': model_name, 'pipeline': model_pipeline}\n",
        "\n",
        "# Print results to verify\n",
        "for idx in results:\n",
        "    print(f\"Index: {idx}, Model Name: {results[idx]['model_name']}, Pipeline: {value['pipeline']}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    nb_fitted_model= train_model(pipeline_nb)\n",
        "    lr_fitted_model = train_model(pipeline_nb)\n",
        "\n",
        "\n",
        "    print(f'Evaluating {model_name}')\n",
        "    train_accuracy, test_accuracy,best_model = evaluate_model()\n",
        "    results[model_name] = {\"train_accuracy\": train_accuracy, \"test_accuracy\": test_accuracy}\n",
        ""
      ],
      "metadata": {
        "id": "8MmY_Fp54MXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xX4pmvAVBByZ",
        "outputId": "843aa553-e419-43ba-dbb3-3ebb1a67f454"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/06/04 20:03:43 WARN TaskSetManager: Stage 216 contains a task of very large size (33450 KiB). The maximum recommended task size is 1000 KiB.\n",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "best_model.save('pp')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ojuCKv1IBBya",
        "outputId": "b8731c56-13c1-49be-bed2-90897f5889d3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/06/04 20:31:50 ERROR Instrumentation: java.lang.IllegalArgumentException: requirement failed: Error loading metadata: Expected class name org.apache.spark.ml.PipelineModel but found class name org.apache.spark.ml.classification.NaiveBayesModel\n",
            "\tat scala.Predef$.require(Predef.scala:281)\n",
            "\tat org.apache.spark.ml.util.DefaultParamsReader$.parseMetadata(ReadWrite.scala:610)\n",
            "\tat org.apache.spark.ml.util.DefaultParamsReader$.loadMetadata(ReadWrite.scala:588)\n",
            "\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$load$3(Pipeline.scala:269)\n",
            "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
            "\tat scala.util.Try$.apply(Try.scala:213)\n",
            "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
            "\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.load(Pipeline.scala:268)\n",
            "\tat org.apache.spark.ml.PipelineModel$PipelineModelReader.$anonfun$load$7(Pipeline.scala:356)\n",
            "\tat org.apache.spark.ml.MLEvents.withLoadInstanceEvent(events.scala:160)\n",
            "\tat org.apache.spark.ml.MLEvents.withLoadInstanceEvent$(events.scala:155)\n",
            "\tat org.apache.spark.ml.util.Instrumentation.withLoadInstanceEvent(Instrumentation.scala:42)\n",
            "\tat org.apache.spark.ml.PipelineModel$PipelineModelReader.$anonfun$load$6(Pipeline.scala:355)\n",
            "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
            "\tat scala.util.Try$.apply(Try.scala:213)\n",
            "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
            "\tat org.apache.spark.ml.PipelineModel$PipelineModelReader.load(Pipeline.scala:355)\n",
            "\tat org.apache.spark.ml.PipelineModel$PipelineModelReader.load(Pipeline.scala:349)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
            "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
            "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
            "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
            "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
            "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
            "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
            "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
            "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
            "\n",
            "24/06/04 20:31:50 ERROR Instrumentation: java.lang.IllegalArgumentException: requirement failed: Error loading metadata: Expected class name org.apache.spark.ml.PipelineModel but found class name org.apache.spark.ml.classification.NaiveBayesModel\n",
            "\tat scala.Predef$.require(Predef.scala:281)\n",
            "\tat org.apache.spark.ml.util.DefaultParamsReader$.parseMetadata(ReadWrite.scala:610)\n",
            "\tat org.apache.spark.ml.util.DefaultParamsReader$.loadMetadata(ReadWrite.scala:588)\n",
            "\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$load$3(Pipeline.scala:269)\n",
            "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
            "\tat scala.util.Try$.apply(Try.scala:213)\n",
            "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
            "\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.load(Pipeline.scala:268)\n",
            "\tat org.apache.spark.ml.PipelineModel$PipelineModelReader.$anonfun$load$7(Pipeline.scala:356)\n",
            "\tat org.apache.spark.ml.MLEvents.withLoadInstanceEvent(events.scala:160)\n",
            "\tat org.apache.spark.ml.MLEvents.withLoadInstanceEvent$(events.scala:155)\n",
            "\tat org.apache.spark.ml.util.Instrumentation.withLoadInstanceEvent(Instrumentation.scala:42)\n",
            "\tat org.apache.spark.ml.PipelineModel$PipelineModelReader.$anonfun$load$6(Pipeline.scala:355)\n",
            "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
            "\tat scala.util.Try$.apply(Try.scala:213)\n",
            "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
            "\tat org.apache.spark.ml.PipelineModel$PipelineModelReader.load(Pipeline.scala:355)\n",
            "\tat org.apache.spark.ml.PipelineModel$PipelineModelReader.load(Pipeline.scala:349)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
            "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
            "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
            "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
            "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
            "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
            "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
            "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
            "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
            "\n"
          ]
        },
        {
          "ename": "IllegalArgumentException",
          "evalue": "requirement failed: Error loading metadata: Expected class name org.apache.spark.ml.PipelineModel but found class name org.apache.spark.ml.classification.NaiveBayesModel",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
            "Input \u001b[0;32mIn [117]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66m# Load the saved pipeline (which includes the Naive Bayes model and feature transformations)\u001b[39m\n\u001b[1;32m      6\u001b[0m pipeline_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpp\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m loaded_pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mPipelineModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66m# Transform the test set using the loaded pipeline\u001b[39m\n\u001b[1;32m     10\u001b[0m predictions \u001b[38;5;241m=\u001b[39m loaded_pipeline\u001b[38;5;241m.\u001b[39mtransform(test_set)\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/ml/util.py:369\u001b[0m, in \u001b[0;36mMLReadable.load\u001b[0;34m(cls, path)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mcls\u001b[39m, path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RL:\n\u001b[1;32m    368\u001b[0m     \u001b[38;5;124m\"\"\"Reads an ML instance from the input path, a shortcut of `read().load(path)`.\"\"\"\u001b[39m\n\u001b[0;32m--> 369\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/ml/pipeline.py:284\u001b[0m, in \u001b[0;36mPipelineModelReader.load\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    282\u001b[0m metadata \u001b[38;5;241m=\u001b[39m DefaultParamsReader\u001b[38;5;241m.\u001b[39mloadMetadata(path, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msc)\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m metadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparamMap\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m metadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparamMap\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mJavaMLReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mType\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mJavaMLReadable[PipelineModel]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcls\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    286\u001b[0m     uid, stages \u001b[38;5;241m=\u001b[39m PipelineSharedReadWrite\u001b[38;5;241m.\u001b[39mload(metadata, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msc, path)\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/ml/util.py:318\u001b[0m, in \u001b[0;36mJavaMLReader.load\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath should be a string, got type \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(path))\n\u001b[0;32m--> 318\u001b[0m java_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clazz, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_java\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis Java ML type cannot be loaded into Python currently: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clazz\n\u001b[1;32m    322\u001b[0m     )\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66m# Hide where the exception came from that shows a non-Pythonic\u001b[39m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66m# JVM exception message.\u001b[39m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
            "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: Error loading metadata: Expected class name org.apache.spark.ml.PipelineModel but found class name org.apache.spark.ml.classification.NaiveBayesModel"
          ]
        }
      ],
      "source": [
        "test_set=te\n",
        "loaded_model = NaiveBayesModel.load('pp')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4gRTQyBBBya"
      },
      "outputs": [],
      "source": [
        "n = 17  # Assuming n is defined somewhere\n",
        "hashingTF = HashingTF(inputCol=\"description_filtered\", outputCol=\"rawFeatures\", numFeatures=2**n)\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "\n",
        "# Apply HashingTF to the test set\n",
        "test_set_with_raw_features = hashingTF.transform(test_set)\n",
        "\n",
        "# Fit IDF on the training set to get the IDF model\n",
        "idf_model = idf.fit(test_set_with_raw_features)\n",
        "\n",
        "# Apply IDF to the test set\n",
        "test_set_with_features = idf_model.transform(test_set_with_raw_features)\n",
        "\n",
        "# Show the test set with the new features column\n",
        "test_set_with_features.select(\"description_filtered\", \"rawFeatures\", \"features\").show(5, truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iW5dii5QBBya",
        "outputId": "644b35bf-1479-4445-bbbf-2947ae26a6c9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r",
            "[Stage 238:>                                                        (0 + 1) / 1]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded model parameters:\n",
            "  featuresCol: features\n",
            "  labelCol: category_label\n",
            "  modelType: multinomial\n",
            "  predictionCol: prediction\n",
            "  probabilityCol: probability\n",
            "  rawPredictionCol: rawPrediction\n",
            "  smoothing: 1.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "loaded_model=NaiveBayesModel.load('pp')\n",
        "# Verify the model is loaded by checking its parameters or making predictions\n",
        "print(\"Loaded model parameters:\")\n",
        "for param, value in loaded_model.extractParamMap().items():\n",
        "    print(f\"  {param.name}: {value}\")\n",
        "\n",
        "# Making predictions on a new dataset\n",
        "# Assuming test_set is your test DataFrame\n",
        "predictions = loaded_model.transform(test_set_with_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8irEBr1BBya",
        "outputId": "76231dad-0286-4e1a-aee9-64e3ed0fa39e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/06/04 20:45:14 WARN DAGScheduler: Broadcasting large task binary with size 34.1 MiB\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+----------+\n",
            "|description_filtered|prediction|\n",
            "+--------------------+----------+\n",
            "|[apple, watch, ev...|      26.0|\n",
            "|[arthveda, fund, ...|       0.0|\n",
            "|[bank, india, hea...|       0.0|\n",
            "|[bank, looking, a...|       0.0|\n",
            "|[bank, may, accel...|       0.0|\n",
            "|[better, liquidit...|       0.0|\n",
            "|[bharti, axa, lif...|       0.0|\n",
            "|[bill, seek, repl...|       0.0|\n",
            "|[central, bank, a...|       0.0|\n",
            "|[cleanup, exercis...|       0.0|\n",
            "|[court, toss, 21,...|      27.0|\n",
            "|[credit, growth, ...|       0.0|\n",
            "|[credit, rating, ...|       0.0|\n",
            "|[deal, ergos, sta...|       0.0|\n",
            "|[describes, brexi...|       2.0|\n",
            "|[draftkings, repo...|       0.0|\n",
            "|[farmer, longer, ...|       0.0|\n",
            "|[financial, intel...|       0.0|\n",
            "|[firstquarter, ad...|       0.0|\n",
            "|[given, poor, fis...|       0.0|\n",
            "+--------------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "predictions.select('description_filtered','prediction').show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSyD5awwBByb"
      },
      "outputs": [],
      "source": [
        "#df.unpersist()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}