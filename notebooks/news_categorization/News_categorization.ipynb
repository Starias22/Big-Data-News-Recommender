{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guYKqw-LBByQ",
        "outputId": "0cb914b5-c60e-4d61-8c9b-7808a57effca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=ce524e6b261d3c4f86eec1ca1cc19106341a686bccf8b498ee4e05bdc4dc2cad\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-qFVy2lBaqu"
      },
      "source": [
        "# News categorization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## I- Modules import"
      ],
      "metadata": {
        "id": "mNfMRWDbFXDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.ml.feature import  IDF, HashingTF\n",
        "from pyspark.ml import  Pipeline\n",
        "from math import ceil,log2\n",
        "from pyspark.ml.classification import LogisticRegression,NaiveBayes,LogisticRegressionModel\n",
        "from pyspark.sql.functions import col,explode,split\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "3QYNQcr-FQLs"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## II- Spark context and session creation"
      ],
      "metadata": {
        "id": "Nj7Qu-EvK9_Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "aFZWn6-QBByS",
        "outputId": "b7430067-1724-411c-c9fb-aa0e1cf8140e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://node02.cm.cluster:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>spark://node02:7077</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>NewsCategorisation</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fff8e1c6250>"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spark = (SparkSession.builder\n",
        "    .master(\"spark://node02:7077\")\n",
        "    .appName(\"NewsCategorisation\")\n",
        "    #.config('spark.driver.cores','4')\n",
        "    #.config(\"spark.executorEnv.LD_LIBRARY_PATH\", \"/home/team1337/.local/easybuild_new/software/Python/3.8.6-GCCcore-10.2.0/lib:/home/team1337/.local/easybuild_new/software/libffi/3.3-GCCcore-10.2.0/lib64:/home/team1337/.local/easybuild_new/software/GMP/6.2.0-GCCcore-10.2.0/lib:/home/team1337/.local/easybuild_new/software/XZ/5.2.5-GCCcore-10.2.0/lib:/home/team1337/.local/easybuild_new/software/SQLite/3.33.0-GCCcore-10.2.0/lib:/home/team1337/.local/easybuild_new/software/Tcl/8.6.10-GCCcore-10.2.0/lib:/home/team1337/.local/easybuild_new/software/libreadline/8.0-GCCcore-10.2.0/lib:/home/team1337/.local/easybuild_new/software/ncurses/6.2-GCCcore-10.2.0/lib:/home/team1337/.local/easybuild_new/software/bzip2/1.0.8-GCCcore-10.2.0/lib:/home/team1337/.local/easybuild_new/software/binutils/2.35-GCCcore-10.2.0/lib:/home/team1337/.local/easybuild_new/software/zlib/1.2.11-GCCcore-10.2.0/lib:/home/team1337/.local/easybuild_new/software/GCCcore/10.2.0/lib64\") \\\n",
        "    #.config(\"spark.pyspark.python\", \"/home/team1337/.local/easybuild_new/software/Python/3.8.6-GCCcore-10.2.0/bin/python3\") \\\n",
        "    .getOrCreate()\n",
        "        )\n",
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## III- Dataframe preparing"
      ],
      "metadata": {
        "id": "PuV9CWoMN60A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Load the data"
      ],
      "metadata": {
        "id": "Em4ZWvqhPoHt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xMI_45McvZto",
        "outputId": "7b2ef733-bdeb-4756-b088-efe035e50be6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r",
            "[Stage 0:>                                                          (0 + 1) / 1]\r",
            "\r",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Load data\n",
        "df = spark.read.parquet(\"input/news.parquet\", header=True, inferSchema=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Partition and cache the dataframe"
      ],
      "metadata": {
        "id": "x4csdl2FOVf8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4EXrJ3GIBByT",
        "outputId": "ddd03fd8-a9c6-436c-abe7-a1828a821abe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.rdd.getNumPartitions()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPBefKKqBByU"
      },
      "outputs": [],
      "source": [
        "num_partitions=5*40\n",
        "df= df.repartition(num_partitions).cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ytFY1tjRBByU",
        "outputId": "fce9d3ee-051f-4b09-b5dd-c8a834dc3e22"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 3:============================================>         (165 + 22) / 200]\r"
          ]
        },
        {
          "data": {
            "text/plain": [
              "200"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.rdd.getNumPartitions()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Preview the data"
      ],
      "metadata": {
        "id": "4G5XozIiQTEx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r86OcjwLv0EU",
        "outputId": "908b16c6-118c-4870-fef3-2dc4086a5ab1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r",
            "                                                                                \r"
          ]
        },
        {
          "data": {
            "text/plain": [
              "1716608"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfy_ivN4BByV",
        "outputId": "047c2814-4f8d-4015-b1bd-ab3e4a3758c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------+--------------------+\n",
            "|category_label|description_filtered|\n",
            "+--------------+--------------------+\n",
            "|          10.0|every bozo need k...|\n",
            "|          11.0|best redness past...|\n",
            "|          10.0|fashion show used...|\n",
            "|           9.0|challenge present...|\n",
            "|          10.0|man vintage show ...|\n",
            "|          11.0|hidden mickey spo...|\n",
            "|          10.0|next fashion uppe...|\n",
            "|          11.0|work home rabbi d...|\n",
            "|           9.0|mommy manage nt g...|\n",
            "|           9.0|ontogenesis hormo...|\n",
            "|          10.0|7 gross grooming ...|\n",
            "|           9.0|period display bo...|\n",
            "|          10.0|transformation ph...|\n",
            "|          10.0|morena baccarin g...|\n",
            "|          11.0|expat recovery ro...|\n",
            "|          11.0|amsterdam diverse...|\n",
            "|          11.0|view afar make ma...|\n",
            "|          10.0|new house york fa...|\n",
            "|           9.0|5 dumbest affair ...|\n",
            "|           9.0|   valentine day kid|\n",
            "+--------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "id": "awWri7zZQcOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Convert filtered descriptions to arrays"
      ],
      "metadata": {
        "id": "VMpXsNNsO_Z6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Drqnhee0i26r",
        "outputId": "ca8e510c-4855-440f-d3e1-a39f6b13a50d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------+----------------------------------------------------------------------------------------------------------------+\n",
            "|category_label|description_filtered                                                                                            |\n",
            "+--------------+----------------------------------------------------------------------------------------------------------------+\n",
            "|10.0          |[every, bozo, need, know, fashion]                                                                              |\n",
            "|11.0          |[best, redness, pasta, sauce, italian, love, life, ve, never, heard]                                            |\n",
            "|10.0          |[fashion, show, used, represent, raucous, liaison, video]                                                       |\n",
            "|9.0           |[challenge, present, girl, constitute, dying]                                                                   |\n",
            "|10.0          |[man, vintage, show, delivers, carefully, curated, habiliment, culled, around, land]                            |\n",
            "|11.0          |[hidden, mickey, spotting, mick, pas, around, walter, elia, disney, world, deception, kingdom, epcot, photo]    |\n",
            "|10.0          |[next, fashion, uppercase, letter, south]                                                                       |\n",
            "|11.0          |[work, home, rabbi, daughter, finally, shuffling, israel, photo]                                                |\n",
            "|9.0           |[mommy, manage, nt, get, want, nursemaid]                                                                       |\n",
            "|9.0           |[ontogenesis, hormone, injectant, subject, logo]                                                                |\n",
            "|10.0          |[7, gross, grooming, mistake, men, make, work]                                                                  |\n",
            "|9.0           |[period, display, board, game, design, destigmatize, menstruation]                                              |\n",
            "|10.0          |[transformation, photo, prove, photoshop, isnt, thing, making, model, look, perfect]                            |\n",
            "|10.0          |[morena, baccarin, golden, globe, dress, 2013, see, red, carpet, aspect, photograph]                            |\n",
            "|11.0          |[expat, recovery, room, bear, blow, travel]                                                                     |\n",
            "|11.0          |[amsterdam, diverse, night, life]                                                                               |\n",
            "|11.0          |[view, afar, make, mardi, gras, new, siege, orleans]                                                            |\n",
            "|10.0          |[new, house, york, fashion, week, front, man, row, famous, person, include, kirsten, dunst, minka, kelly, photo]|\n",
            "|9.0           |[5, dumbest, affair, first, two, year, beginner]                                                                |\n",
            "|9.0           |[valentine, day, kid]                                                                                           |\n",
            "+--------------+----------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Create a new DataFrame with description_filtered as arrays\n",
        "df= df.withColumn('description_filtered', split(col('description_filtered'), ' '))\n",
        "# Show the new DataFrame\n",
        "df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IV- Feature Engineering\n"
      ],
      "metadata": {
        "id": "3RmBazOpQqXF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Explode the filtered descriptions to get the words"
      ],
      "metadata": {
        "id": "LjsTG0YLSyEn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qON2vCKBByX"
      },
      "outputs": [],
      "source": [
        "exploded_df=df.select(explode(df.description_filtered)).alias('words')\n",
        "exploded_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DH41jOsrBByX"
      },
      "outputs": [],
      "source": [
        "#df=df.unpersist()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Get unique words in the filtered_description"
      ],
      "metadata": {
        "id": "MohTRQirTXD3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NbJFyQlBByX"
      },
      "outputs": [],
      "source": [
        "unique_words=exploded_df.distinct()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Cache and show the unique words dataframe"
      ],
      "metadata": {
        "id": "t-UjxthKU0Rq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1S8h39xBByX"
      },
      "outputs": [],
      "source": [
        "unique_words=unique_words.cache()\n",
        "unique_words.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Get the vocabulary size"
      ],
      "metadata": {
        "id": "KCb1pLl3VLOL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCakXZ3PBByX",
        "outputId": "c391ee19-f5c6-4a9e-cc53-4c4cd0b031e1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "128622"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocabulary_size=unique_words.count()\n",
        "vocabulary_size"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Unpersit the unique words dataframe(not needed anymore)"
      ],
      "metadata": {
        "id": "ycUp4mQDVbZ1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nS-8eBNnBByX"
      },
      "outputs": [],
      "source": [
        "unique_words=unique_words.unpersist()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Get the smallest `n` such that $2^n$ is greater than `vocabulary_size`"
      ],
      "metadata": {
        "id": "irYncNP3WWhr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_features=log2(vocabulary_size)\n"
      ],
      "metadata": {
        "id": "9kdafIt4WnP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0nzNFsoBByY",
        "outputId": "83b3aebf-db97-48f0-df3e-46aa7c20ea0b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "17"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "n=ceil(log2(vocabulary_size))\n",
        "n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Get the number of features for HashingTF"
      ],
      "metadata": {
        "id": "mtwfZn61Yzh-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_features=2**n\n",
        "num_features"
      ],
      "metadata": {
        "id": "FviyZsE_YxxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Define the HashingTF and IDF stages"
      ],
      "metadata": {
        "id": "cV9aikKbYQzj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the HashingTF and IDF stages\n",
        "hashingTF = HashingTF(inputCol=\"description_filtered\", outputCol=\"rawFeatures\", numFeatures=num_features)\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")"
      ],
      "metadata": {
        "id": "bIL-PyIHYm4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## V- Models set up, training and evaluation"
      ],
      "metadata": {
        "id": "74T1zTMRxKP8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Set up Naive and Logistic regression classifiers"
      ],
      "metadata": {
        "id": "HmY4gqj5Q2w_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the classifiers\n",
        "\n",
        "# Logistic regression classifier\n",
        "lr = LogisticRegression(labelCol=\"category_label\", featuresCol=\"features\")\n",
        "\n",
        "# Naive Bayes classifier\n",
        "nb = NaiveBayes(labelCol=\"category_label\", featuresCol=\"features\")"
      ],
      "metadata": {
        "id": "f8ya3gJ_PxLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Set up pipelines\n",
        "\n",
        "We will  set up and returns  the pipelines of the following transformations for Native Bayes and Linear reggression\n",
        "\n",
        "- HashingTF\n",
        "- IDF\n",
        "- 3-Fold Cross-validation  without grid search"
      ],
      "metadata": {
        "id": "XT81OlVr5Ck1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LArI4unEDer1",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Define parameter grids\n",
        "paramGrid_nb = (ParamGridBuilder()\n",
        "        .addGrid(nb.smoothing, [0.5, 1.0, 2.0])\n",
        "        .build())\n",
        "\n",
        "paramGrid_nb=paramGrid_lr=ParamGridBuilder().build()\n",
        "\n",
        "# Create cross validators\n",
        "\n",
        "# Cross-validation for Naive Bayes\n",
        "cv_nb = CrossValidator(estimator=nb, estimatorParamMaps=paramGrid_nb,\n",
        "                        evaluator=MulticlassClassificationEvaluator(labelCol=\"category_label\", predictionCol=\"prediction\", metricName=\"accuracy\"),\n",
        "                        numFolds=3, parallelism=1)\n",
        "# Cross-validation for Logistic Regression\n",
        "cv_lr = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid_lr,\n",
        "                        evaluator=MulticlassClassificationEvaluator(labelCol=\"category_label\", predictionCol=\"prediction\", metricName=\"accuracy\"),\n",
        "                        numFolds=3, parallelism=1)\n",
        "\n",
        "\n",
        "# Create pipelines\n",
        "# Pipeline for Naive Bayes\n",
        "pipeline_nb = Pipeline(stages=[hashingTF, idf, cv_nb])\n",
        "# Pipeline for Logistic Regression\n",
        "pipeline_lr = Pipeline(stages=[hashingTF, idf, cv_lr])\n",
        "model_pipelines=pipeline_nb, pipeline_lr\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Split the data\n",
        "\n",
        "First of all let us split the data into train and test set: 80% for train and 20% for test"
      ],
      "metadata": {
        "id": "UD0gwMNf7yY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data\n",
        "(train_set, test_set) = df.randomSplit([0.8, 0.2], seed=0)"
      ],
      "metadata": {
        "id": "pThnAKRy8LqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Create a function for model training\n",
        "\n",
        "Let us create a function which takes as argument a model that it trains and then returns the trained model."
      ],
      "metadata": {
        "id": "M84xeE8i67YF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model):\n",
        "    print('Training the model')\n",
        "    fitted_model = model.fit(train_set)\n",
        "    print('Done')\n",
        "    return fitted_model"
      ],
      "metadata": {
        "id": "AvRF-E1-7ncY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Define a function to evaluate the model\n",
        "\n",
        "The function takes as parameter a fitted model, evaluates the model on train and test split and then return the train and test performance. The accuracy is the metric used."
      ],
      "metadata": {
        "id": "K-fBEB1f9aRD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIKVzBw1BByY"
      },
      "outputs": [],
      "source": [
        "# Initialize the evaluator\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"category_label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "\n",
        "# Function to evaluate model and get best parameters\n",
        "def evaluate_model(fitted_model):\n",
        "\n",
        "    print('Making predictions on the training set')\n",
        "\n",
        "    train_predictions = fitted_model.transform(train_set)\n",
        "\n",
        "    print('Making predictions on the test set')\n",
        "    test_predictions = fitted_model.transform(test_set)\n",
        "\n",
        "    print('Evaluating the model on training set')\n",
        "    train_accuracy = evaluator.evaluate(train_predictions)\n",
        "\n",
        "    print('Evaluating the model on test set')\n",
        "    test_accuracy = evaluator.evaluate(test_predictions)\n",
        "    return train_accuracy, test_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to evaluate model and get best parameters\n",
        "def evaluate_model(model, model_name):\n",
        "    print('Training the model')\n",
        "\n",
        "    # Train the model using cross-validation\n",
        "    fitted_model = model.fit(train_set)\n",
        "\n",
        "    # Get the best model from cross-validation\n",
        "    best_model = fitted_model.stages[-1].bestModel\n",
        "\n",
        "    print('Making predictions on the training set')\n",
        "    # Make predictions on the training set\n",
        "    train_predictions = fitted_model.transform(train_set)\n",
        "\n",
        "    print('Making predictions on the test set')\n",
        "    # Make predictions on the test set\n",
        "    test_predictions = fitted_model.transform(test_set)\n",
        "\n",
        "    # Initialize the evaluator\n",
        "    evaluator = MulticlassClassificationEvaluator(labelCol=\"category_label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "\n",
        "    print('Evaluating the model on training set')\n",
        "    # Evaluate the model on the training set\n",
        "    train_accuracy = evaluator.evaluate(train_predictions)\n",
        "\n",
        "    print('Evaluating the model on test set')\n",
        "    # Evaluate the model on the test set\n",
        "    test_accuracy = evaluator.evaluate(test_predictions)\n",
        "\n",
        "    print(f\"{model_name} Train Accuracy: {train_accuracy}\")\n",
        "    print(f\"{model_name} Test Accuracy: {test_accuracy}\")\n",
        "\n",
        "    # Print the best parameters\n",
        "    print(f\"Best parameters for {model_name}:\")\n",
        "\n",
        "    for param, value in best_model.extractParamMap().items():\n",
        "        print(f\"  {param.name}: {value}\")\n",
        "\n",
        "    return train_accuracy, test_accuracy,best_model"
      ],
      "metadata": {
        "id": "eJHhpCCj-jCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Create a function which takes pipelines and train the models, evaluate them and then return the results"
      ],
      "metadata": {
        "id": "yG6dMBjzLXWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate_models(model_pipelines,model_names=[\"Naive Bayes\", \"Logistic Regression\"]):\n",
        "\n",
        "    # Initialize the results dictionary\n",
        "    results = {}\n",
        "\n",
        "    # Loop over the indices and model names simultaneously\n",
        "    for idx, (model_pipeline, model_name) in enumerate(zip(model_pipelines, model_names)):\n",
        "        print(f\"Training {model_name} model\")\n",
        "\n",
        "        # Fit the model pipeline to the training set\n",
        "        fitted_model = model_pipeline.fit(train_set)\n",
        "\n",
        "        print(\"Done\")\n",
        "        print(f\"Evaluating {model_name} model\")\n",
        "\n",
        "        # Evaluate the fitted model\n",
        "        train_accuracy, test_accuracy, best_model = evaluate_model(fitted_model)\n",
        "\n",
        "        # Store the results\n",
        "        results[idx] = {\n",
        "            'model_name': model_name,\n",
        "            #'pipeline': model_pipeline,\n",
        "            'fitted_model': fitted_model,\n",
        "            \"train_accuracy\": train_accuracy}\n",
        "\n",
        "        if len(model_name)==0:\n",
        "            results=results[0]\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "8MmY_Fp54MXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Call the function and interpret the results"
      ],
      "metadata": {
        "id": "pYDcrjUHBQHf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = train_and_evaluate_models()"
      ],
      "metadata": {
        "id": "9T4JAj-QId2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results[0]"
      ],
      "metadata": {
        "id": "ycLhdvlMJ8nB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results[1]"
      ],
      "metadata": {
        "id": "OQNk6X_RKBuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We remark that\n",
        "- Naive Bayes\n",
        "- Logistic regression\n",
        "\n",
        "We can then conclude that t\n",
        "- he two models set a good performance on both training and test set.\n",
        "- The Logistic regression models outperforms the Naive Bayes model\n",
        "\n",
        "In the next section, we will tune the parameters of the Naive bayes to get the best parameters."
      ],
      "metadata": {
        "id": "808-IeqCNb1v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VI- Logistic regression hyperparameters tuning"
      ],
      "metadata": {
        "id": "xkME6ByNOegG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Pipeline creation"
      ],
      "metadata": {
        "id": "pAWi0NqwSzgp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parameter grids for Logistic regresion grid search\n",
        "reg_values = np.logspace(-4, 4, num=100)\n",
        "l1_ratios = np.linspace(0, 1, num=10)\n",
        "\n",
        "paramGrid_lr=paramGrid_lr.addGrid(lr.regParam, reg_values).build()\n",
        "\n",
        "# Create Cross-validation for Logistic Regression\n",
        "cv_lr = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid_lr,\n",
        "                        evaluator=MulticlassClassificationEvaluator(labelCol=\"category_label\", predictionCol=\"prediction\", metricName=\"accuracy\"),\n",
        "                        numFolds=3, parallelism=1)\n",
        "\n",
        "\n",
        "# Create pipeline for Logistic Regression\n",
        "pipeline_lr = Pipeline(stages=[hashingTF, idf, cv_lr])\n",
        "\n",
        "pipeline_lr"
      ],
      "metadata": {
        "id": "Wtq5K0JbOdpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Hyperparameters tuning"
      ],
      "metadata": {
        "id": "0XhMJxEzS87h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results=train_and_evaluate_models(model_pipelines=[pipeline_lr],model_names=[\"Logistic Regression\"])\n",
        "results"
      ],
      "metadata": {
        "id": "4bSLe3K5TL1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Get the best parameters"
      ],
      "metadata": {
        "id": "yUlLvsWVTkDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fitted_model=results['fitted_model']\n",
        "\n",
        "# Get the best model\n",
        "best_model = fitted_model.stages[-1].bestModel\n",
        "\n",
        "# Print the best parameters\n",
        "print(f\"Best parameters for Logistic regression:\")\n",
        "\n",
        "for param, value in best_model.extractParamMap().items():\n",
        "     print(f\"  {param.name}: {value}\")"
      ],
      "metadata": {
        "id": "k5rCb_boTk3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Save the best model"
      ],
      "metadata": {
        "id": "uEH0OfIqUxOg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xX4pmvAVBByZ",
        "outputId": "843aa553-e419-43ba-dbb3-3ebb1a67f454"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/06/04 20:03:43 WARN TaskSetManager: Stage 216 contains a task of very large size (33450 KiB). The maximum recommended task size is 1000 KiB.\n",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "best_model.save('output/news_categorization_model')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VII- Summary\n",
        "\n",
        "In this notebook we have studied two models for our news categorization task. There are Naive Bayes and Logistic regression.\n",
        "\n",
        " Our study reveals that the Logistic regression was the one with best performance.\n",
        "\n",
        " Then we tunned the Logistic regression hyperparameters using grid search and then we find the best model that we save.\n",
        "\n",
        " The next step of our work will be to ..."
      ],
      "metadata": {
        "id": "GnFsw8x5Vyy2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSyD5awwBByb"
      },
      "outputs": [],
      "source": [
        "#df.unpersist()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}