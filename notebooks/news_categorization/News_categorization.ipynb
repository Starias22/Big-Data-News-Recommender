{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "guYKqw-LBByQ"
      },
      "outputs": [],
      "source": [
        "#spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-qFVy2lBaqu"
      },
      "source": [
        "# News categorization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## I- Modules import"
      ],
      "metadata": {
        "id": "mNfMRWDbFXDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.ml.feature import  IDF, HashingTF,Pipeline\n",
        "from math import ceil,log2\n",
        "from pyspark.ml.classification import LogisticRegression,NaiveBayes,LogisticRegressionModel\n",
        "from pyspark.sql.functions import col,explode,split"
      ],
      "metadata": {
        "id": "3QYNQcr-FQLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## II- Spark context and session creation"
      ],
      "metadata": {
        "id": "Nj7Qu-EvK9_Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "aFZWn6-QBByS",
        "outputId": "b7430067-1724-411c-c9fb-aa0e1cf8140e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://node02.cm.cluster:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>spark://node02:7077</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>NewsCategorisation</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fff8e1c6250>"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spark = (SparkSession.builder\n",
        "    .master(\"spark://node02:7077\")\n",
        "    .appName(\"NewsCategorisation\")\n",
        "    #.config('spark.driver.cores','4')\n",
        "    #.config(\"spark.executorEnv.LD_LIBRARY_PATH\", \"/home/team1337/.local/easybuild_new/software/Python/3.8.6-GCCcore-10.2.0/lib:/home/team1337/.local/easybuild_new/software/libffi/3.3-GCCcore-10.2.0/lib64:/home/team1337/.local/easybuild_new/software/GMP/6.2.0-GCCcore-10.2.0/lib:/home/team1337/.local/easybuild_new/software/XZ/5.2.5-GCCcore-10.2.0/lib:/home/team1337/.local/easybuild_new/software/SQLite/3.33.0-GCCcore-10.2.0/lib:/home/team1337/.local/easybuild_new/software/Tcl/8.6.10-GCCcore-10.2.0/lib:/home/team1337/.local/easybuild_new/software/libreadline/8.0-GCCcore-10.2.0/lib:/home/team1337/.local/easybuild_new/software/ncurses/6.2-GCCcore-10.2.0/lib:/home/team1337/.local/easybuild_new/software/bzip2/1.0.8-GCCcore-10.2.0/lib:/home/team1337/.local/easybuild_new/software/binutils/2.35-GCCcore-10.2.0/lib:/home/team1337/.local/easybuild_new/software/zlib/1.2.11-GCCcore-10.2.0/lib:/home/team1337/.local/easybuild_new/software/GCCcore/10.2.0/lib64\") \\\n",
        "    #.config(\"spark.pyspark.python\", \"/home/team1337/.local/easybuild_new/software/Python/3.8.6-GCCcore-10.2.0/bin/python3\") \\\n",
        "    .getOrCreate()\n",
        "        )\n",
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xMI_45McvZto",
        "outputId": "7b2ef733-bdeb-4756-b088-efe035e50be6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r",
            "[Stage 0:>                                                          (0 + 1) / 1]\r",
            "\r",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Load data\n",
        "#df = spark.read.csv(\"input/news.csv\", header=True, inferSchema=True)\n",
        "df = spark.read.parquet(\"input/news.parquet\", header=True, inferSchema=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4EXrJ3GIBByT",
        "outputId": "ddd03fd8-a9c6-436c-abe7-a1828a821abe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.rdd.getNumPartitions()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPBefKKqBByU"
      },
      "outputs": [],
      "source": [
        "num_partitions=5*40\n",
        "df= df.repartition(num_partitions).cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ytFY1tjRBByU",
        "outputId": "fce9d3ee-051f-4b09-b5dd-c8a834dc3e22"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 3:============================================>         (165 + 22) / 200]\r"
          ]
        },
        {
          "data": {
            "text/plain": [
              "200"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.rdd.getNumPartitions()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lyi-2yH0BByU",
        "outputId": "4ae8a94a-59af-4cca-8f18-05b496397f6e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/06/04 19:20:53 WARN CacheManager: Asked to cache already cached data.\n"
          ]
        }
      ],
      "source": [
        "df=df.cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r86OcjwLv0EU",
        "outputId": "908b16c6-118c-4870-fef3-2dc4086a5ab1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r",
            "                                                                                \r"
          ]
        },
        {
          "data": {
            "text/plain": [
              "1716608"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfy_ivN4BByV",
        "outputId": "047c2814-4f8d-4015-b1bd-ab3e4a3758c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------+--------------------+\n",
            "|category_label|description_filtered|\n",
            "+--------------+--------------------+\n",
            "|          10.0|every bozo need k...|\n",
            "|          11.0|best redness past...|\n",
            "|          10.0|fashion show used...|\n",
            "|           9.0|challenge present...|\n",
            "|          10.0|man vintage show ...|\n",
            "|          11.0|hidden mickey spo...|\n",
            "|          10.0|next fashion uppe...|\n",
            "|          11.0|work home rabbi d...|\n",
            "|           9.0|mommy manage nt g...|\n",
            "|           9.0|ontogenesis hormo...|\n",
            "|          10.0|7 gross grooming ...|\n",
            "|           9.0|period display bo...|\n",
            "|          10.0|transformation ph...|\n",
            "|          10.0|morena baccarin g...|\n",
            "|          11.0|expat recovery ro...|\n",
            "|          11.0|amsterdam diverse...|\n",
            "|          11.0|view afar make ma...|\n",
            "|          10.0|new house york fa...|\n",
            "|           9.0|5 dumbest affair ...|\n",
            "|           9.0|   valentine day kid|\n",
            "+--------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Drqnhee0i26r",
        "outputId": "ca8e510c-4855-440f-d3e1-a39f6b13a50d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------+----------------------------------------------------------------------------------------------------------------+\n",
            "|category_label|description_filtered                                                                                            |\n",
            "+--------------+----------------------------------------------------------------------------------------------------------------+\n",
            "|10.0          |[every, bozo, need, know, fashion]                                                                              |\n",
            "|11.0          |[best, redness, pasta, sauce, italian, love, life, ve, never, heard]                                            |\n",
            "|10.0          |[fashion, show, used, represent, raucous, liaison, video]                                                       |\n",
            "|9.0           |[challenge, present, girl, constitute, dying]                                                                   |\n",
            "|10.0          |[man, vintage, show, delivers, carefully, curated, habiliment, culled, around, land]                            |\n",
            "|11.0          |[hidden, mickey, spotting, mick, pas, around, walter, elia, disney, world, deception, kingdom, epcot, photo]    |\n",
            "|10.0          |[next, fashion, uppercase, letter, south]                                                                       |\n",
            "|11.0          |[work, home, rabbi, daughter, finally, shuffling, israel, photo]                                                |\n",
            "|9.0           |[mommy, manage, nt, get, want, nursemaid]                                                                       |\n",
            "|9.0           |[ontogenesis, hormone, injectant, subject, logo]                                                                |\n",
            "|10.0          |[7, gross, grooming, mistake, men, make, work]                                                                  |\n",
            "|9.0           |[period, display, board, game, design, destigmatize, menstruation]                                              |\n",
            "|10.0          |[transformation, photo, prove, photoshop, isnt, thing, making, model, look, perfect]                            |\n",
            "|10.0          |[morena, baccarin, golden, globe, dress, 2013, see, red, carpet, aspect, photograph]                            |\n",
            "|11.0          |[expat, recovery, room, bear, blow, travel]                                                                     |\n",
            "|11.0          |[amsterdam, diverse, night, life]                                                                               |\n",
            "|11.0          |[view, afar, make, mardi, gras, new, siege, orleans]                                                            |\n",
            "|10.0          |[new, house, york, fashion, week, front, man, row, famous, person, include, kirsten, dunst, minka, kelly, photo]|\n",
            "|9.0           |[5, dumbest, affair, first, two, year, beginner]                                                                |\n",
            "|9.0           |[valentine, day, kid]                                                                                           |\n",
            "+--------------+----------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Create a new DataFrame with description_filtered as arrays\n",
        "df= df.withColumn('description_filtered', split(col('description_filtered'), ' '))\n",
        "# Show the new DataFrame\n",
        "df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qON2vCKBByX"
      },
      "outputs": [],
      "source": [
        "exploded_df=df.select(explode(df.description_filtered)).alias('words')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DH41jOsrBByX"
      },
      "outputs": [],
      "source": [
        "#df=df.unpersist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NbJFyQlBByX"
      },
      "outputs": [],
      "source": [
        "unique_words=exploded_df.distinct()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1S8h39xBByX"
      },
      "outputs": [],
      "source": [
        "unique_words=unique_words.cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJvW6GoeBByX",
        "outputId": "1ce2825f-e44e-4602-9902-e62035211bd2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 14:================================================>    (184 + 12) / 200]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+\n",
            "|     col|\n",
            "+--------+\n",
            "|     art|\n",
            "|   oscar|\n",
            "|newlywed|\n",
            "|  harder|\n",
            "|  teigen|\n",
            "|   still|\n",
            "|   monte|\n",
            "|   carlo|\n",
            "| hitched|\n",
            "|  filing|\n",
            "|    pant|\n",
            "|  travel|\n",
            "|  bazaar|\n",
            "|  poetry|\n",
            "|  outfit|\n",
            "|   1970s|\n",
            "| elevate|\n",
            "|  online|\n",
            "| embrace|\n",
            "|  patton|\n",
            "+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r",
            "[Stage 17:==============================================>      (176 + 20) / 200]\r",
            "\r",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "unique_words.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCakXZ3PBByX",
        "outputId": "c391ee19-f5c6-4a9e-cc53-4c4cd0b031e1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "128622"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocabulary_size=unique_words.count()\n",
        "vocabulary_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nS-8eBNnBByX"
      },
      "outputs": [],
      "source": [
        "unique_words=unique_words.unpersist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0nzNFsoBByY",
        "outputId": "83b3aebf-db97-48f0-df3e-46aa7c20ea0b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "17"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "n=ceil(log2(vocabulary_size))\n",
        "n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIKVzBw1BByY"
      },
      "outputs": [],
      "source": [
        "# Function to evaluate model and get best parameters\n",
        "def evaluate_model(model, model_name):\n",
        "    print('Training the model')\n",
        "\n",
        "    # Train the model using cross-validation\n",
        "    fitted_model = model.fit(train_set)\n",
        "\n",
        "    # Get the best model from cross-validation\n",
        "    best_model = fitted_model.stages[-1].bestModel\n",
        "\n",
        "    print('Making predictions on the training set')\n",
        "    # Make predictions on the training set\n",
        "    train_predictions = fitted_model.transform(train_set)\n",
        "\n",
        "    print('Making predictions on the test set')\n",
        "    # Make predictions on the test set\n",
        "    test_predictions = fitted_model.transform(test_set)\n",
        "\n",
        "    # Initialize the evaluator\n",
        "    evaluator = MulticlassClassificationEvaluator(labelCol=\"category_label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "\n",
        "    print('Evaluating the model on training set')\n",
        "    # Evaluate the model on the training set\n",
        "    train_accuracy = evaluator.evaluate(train_predictions)\n",
        "\n",
        "    print('Evaluating the model on test set')\n",
        "    # Evaluate the model on the test set\n",
        "    test_accuracy = evaluator.evaluate(test_predictions)\n",
        "\n",
        "    print(f\"{model_name} Train Accuracy: {train_accuracy}\")\n",
        "    print(f\"{model_name} Test Accuracy: {test_accuracy}\")\n",
        "\n",
        "    # Print the best parameters\n",
        "    print(f\"Best parameters for {model_name}:\")\n",
        "\n",
        "    for param, value in best_model.extractParamMap().items():\n",
        "        print(f\"  {param.name}: {value}\")\n",
        "\n",
        "    return train_accuracy, test_accuracy,best_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LArI4unEDer1",
        "outputId": "6afe75e1-d6fb-4175-84ea-1273193556ed",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating Naive Bayes\n",
            "Training the model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/06/04 19:56:22 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
            "24/06/04 19:56:37 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
            "24/06/04 19:56:39 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
            "24/06/04 19:56:44 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
            "24/06/04 19:56:45 WARN DAGScheduler: Broadcasting large task binary with size 34.1 MiB\n",
            "24/06/04 19:56:47 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
            "24/06/04 19:56:48 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
            "24/06/04 19:56:51 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
            "24/06/04 19:56:55 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
            "24/06/04 19:56:57 WARN DAGScheduler: Broadcasting large task binary with size 34.1 MiB\n",
            "24/06/04 19:56:58 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
            "24/06/04 19:56:59 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
            "24/06/04 19:57:02 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
            "24/06/04 19:57:06 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
            "24/06/04 19:57:08 WARN DAGScheduler: Broadcasting large task binary with size 34.1 MiB\n",
            "24/06/04 19:57:09 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
            "24/06/04 19:57:13 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Making predictions on the training set\n",
            "Making predictions on the test set\n",
            "Evaluating the model on training set\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/06/04 19:57:17 WARN DAGScheduler: Broadcasting large task binary with size 34.1 MiB\n",
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating the model on test set\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/06/04 19:57:23 WARN DAGScheduler: Broadcasting large task binary with size 34.1 MiB\n",
            "[Stage 213:===========================================>        (168 + 20) / 200]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Naive Bayes Train Accuracy: 0.786215323314107\n",
            "Naive Bayes Test Accuracy: 0.7531130303074436\n",
            "Best parameters for Naive Bayes:\n",
            "  featuresCol: features\n",
            "  labelCol: category_label\n",
            "  modelType: multinomial\n",
            "  predictionCol: prediction\n",
            "  probabilityCol: probability\n",
            "  rawPredictionCol: rawPrediction\n",
            "  smoothing: 1.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r",
            "[Stage 213:====================================================>(198 + 2) / 200]\r",
            "\r",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Define the HashingTF and IDF stages\n",
        "hashingTF = HashingTF(inputCol=\"description_filtered\", outputCol=\"rawFeatures\", numFeatures=2**n)\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "\n",
        "# Define the classifiers\n",
        "lr = LogisticRegression(labelCol=\"category_label\", featuresCol=\"features\")\n",
        "\n",
        "nb = NaiveBayes(labelCol=\"category_label\", featuresCol=\"features\")\n",
        "\n",
        "# Define parameter grids\n",
        "paramGrid_nb = ParamGridBuilder() \\\n",
        "    .addGrid(nb.smoothing, [0.5, 1.0, 2.0]) \\\n",
        "    .build()\n",
        "\n",
        "\n",
        "\n",
        "reg_values = np.logspace(-4, 4, num=100)\n",
        "\n",
        "l1_ratios = np.linspace(0, 1, num=10)\n",
        "\n",
        "paramGrid_lr = (ParamGridBuilder()\n",
        "    .addGrid(lr.regParam, reg_values)\n",
        "    #.addGrid(lr.elasticNetParam, l1_ratios)\n",
        "    .build())\n",
        "\n",
        "\n",
        "\n",
        "paramGrid_dt = (ParamGridBuilder()\n",
        "    #.addGrid(dt.numTrees, [10, 50, 100])\n",
        "    .addGrid(dt.maxDepth, [5, 10, 20])\n",
        "    .build())\n",
        "\n",
        "# Create cross validators\n",
        "cv_nb = CrossValidator(estimator=nb, estimatorParamMaps=ParamGridBuilder().build(), #paramGrid_nb,\n",
        "                       evaluator=MulticlassClassificationEvaluator(labelCol=\"category_label\", predictionCol=\"prediction\", metricName=\"accuracy\"),\n",
        "                       numFolds=3, parallelism=1)\n",
        "\n",
        "cv_lr = CrossValidator(estimator=lr, estimatorParamMaps=ParamGridBuilder().build(), #paramGrid_lr,\n",
        "                       evaluator=MulticlassClassificationEvaluator(labelCol=\"category_label\", predictionCol=\"prediction\", metricName=\"accuracy\"),\n",
        "                       numFolds=3, parallelism=1)\n",
        "\n",
        "cv_dt = CrossValidator(estimator=dt, estimatorParamMaps=ParamGridBuilder().build(), # paramGrid_rf,\n",
        "                       evaluator=MulticlassClassificationEvaluator(labelCol=\"category_label\", predictionCol=\"prediction\", metricName=\"accuracy\"),\n",
        "                       numFolds=3,parallelism=3)\n",
        "\n",
        "# Create pipelines\n",
        "pipeline_nb = Pipeline(stages=[hashingTF, idf, cv_nb])\n",
        "pipeline_lr = Pipeline(stages=[hashingTF, idf, cv_lr])\n",
        "\n",
        "# Split data\n",
        "(train_set, test_set) = df.randomSplit([0.8, 0.2], seed=0)\n",
        "\n",
        "\n",
        "\n",
        "# Evaluate models using the evaluate_model function\n",
        "results = {}\n",
        "for model_pipeline, model_name in [\n",
        "    (pipeline_nb, \"Naive Bayes\"),\n",
        "    #(pipeline_lr, \"Logistic Regression\"),\n",
        "\n",
        "]:\n",
        "    print(f'Evaluating {model_name}')\n",
        "    train_accuracy, test_accuracy,best_model = evaluate_model(model_pipeline, model_name)\n",
        "    results[model_name] = {\"train_accuracy\": train_accuracy, \"test_accuracy\": test_accuracy}\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xX4pmvAVBByZ",
        "outputId": "843aa553-e419-43ba-dbb3-3ebb1a67f454"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/06/04 20:03:43 WARN TaskSetManager: Stage 216 contains a task of very large size (33450 KiB). The maximum recommended task size is 1000 KiB.\n",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "best_model.save('pp')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ojuCKv1IBBya",
        "outputId": "b8731c56-13c1-49be-bed2-90897f5889d3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/06/04 20:31:50 ERROR Instrumentation: java.lang.IllegalArgumentException: requirement failed: Error loading metadata: Expected class name org.apache.spark.ml.PipelineModel but found class name org.apache.spark.ml.classification.NaiveBayesModel\n",
            "\tat scala.Predef$.require(Predef.scala:281)\n",
            "\tat org.apache.spark.ml.util.DefaultParamsReader$.parseMetadata(ReadWrite.scala:610)\n",
            "\tat org.apache.spark.ml.util.DefaultParamsReader$.loadMetadata(ReadWrite.scala:588)\n",
            "\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$load$3(Pipeline.scala:269)\n",
            "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
            "\tat scala.util.Try$.apply(Try.scala:213)\n",
            "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
            "\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.load(Pipeline.scala:268)\n",
            "\tat org.apache.spark.ml.PipelineModel$PipelineModelReader.$anonfun$load$7(Pipeline.scala:356)\n",
            "\tat org.apache.spark.ml.MLEvents.withLoadInstanceEvent(events.scala:160)\n",
            "\tat org.apache.spark.ml.MLEvents.withLoadInstanceEvent$(events.scala:155)\n",
            "\tat org.apache.spark.ml.util.Instrumentation.withLoadInstanceEvent(Instrumentation.scala:42)\n",
            "\tat org.apache.spark.ml.PipelineModel$PipelineModelReader.$anonfun$load$6(Pipeline.scala:355)\n",
            "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
            "\tat scala.util.Try$.apply(Try.scala:213)\n",
            "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
            "\tat org.apache.spark.ml.PipelineModel$PipelineModelReader.load(Pipeline.scala:355)\n",
            "\tat org.apache.spark.ml.PipelineModel$PipelineModelReader.load(Pipeline.scala:349)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
            "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
            "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
            "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
            "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
            "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
            "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
            "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
            "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
            "\n",
            "24/06/04 20:31:50 ERROR Instrumentation: java.lang.IllegalArgumentException: requirement failed: Error loading metadata: Expected class name org.apache.spark.ml.PipelineModel but found class name org.apache.spark.ml.classification.NaiveBayesModel\n",
            "\tat scala.Predef$.require(Predef.scala:281)\n",
            "\tat org.apache.spark.ml.util.DefaultParamsReader$.parseMetadata(ReadWrite.scala:610)\n",
            "\tat org.apache.spark.ml.util.DefaultParamsReader$.loadMetadata(ReadWrite.scala:588)\n",
            "\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$load$3(Pipeline.scala:269)\n",
            "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
            "\tat scala.util.Try$.apply(Try.scala:213)\n",
            "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
            "\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.load(Pipeline.scala:268)\n",
            "\tat org.apache.spark.ml.PipelineModel$PipelineModelReader.$anonfun$load$7(Pipeline.scala:356)\n",
            "\tat org.apache.spark.ml.MLEvents.withLoadInstanceEvent(events.scala:160)\n",
            "\tat org.apache.spark.ml.MLEvents.withLoadInstanceEvent$(events.scala:155)\n",
            "\tat org.apache.spark.ml.util.Instrumentation.withLoadInstanceEvent(Instrumentation.scala:42)\n",
            "\tat org.apache.spark.ml.PipelineModel$PipelineModelReader.$anonfun$load$6(Pipeline.scala:355)\n",
            "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
            "\tat scala.util.Try$.apply(Try.scala:213)\n",
            "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
            "\tat org.apache.spark.ml.PipelineModel$PipelineModelReader.load(Pipeline.scala:355)\n",
            "\tat org.apache.spark.ml.PipelineModel$PipelineModelReader.load(Pipeline.scala:349)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
            "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
            "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
            "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
            "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
            "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
            "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
            "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
            "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
            "\n"
          ]
        },
        {
          "ename": "IllegalArgumentException",
          "evalue": "requirement failed: Error loading metadata: Expected class name org.apache.spark.ml.PipelineModel but found class name org.apache.spark.ml.classification.NaiveBayesModel",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
            "Input \u001b[0;32mIn [117]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66m# Load the saved pipeline (which includes the Naive Bayes model and feature transformations)\u001b[39m\n\u001b[1;32m      6\u001b[0m pipeline_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpp\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m loaded_pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mPipelineModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66m# Transform the test set using the loaded pipeline\u001b[39m\n\u001b[1;32m     10\u001b[0m predictions \u001b[38;5;241m=\u001b[39m loaded_pipeline\u001b[38;5;241m.\u001b[39mtransform(test_set)\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/ml/util.py:369\u001b[0m, in \u001b[0;36mMLReadable.load\u001b[0;34m(cls, path)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mcls\u001b[39m, path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RL:\n\u001b[1;32m    368\u001b[0m     \u001b[38;5;124m\"\"\"Reads an ML instance from the input path, a shortcut of `read().load(path)`.\"\"\"\u001b[39m\n\u001b[0;32m--> 369\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/ml/pipeline.py:284\u001b[0m, in \u001b[0;36mPipelineModelReader.load\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    282\u001b[0m metadata \u001b[38;5;241m=\u001b[39m DefaultParamsReader\u001b[38;5;241m.\u001b[39mloadMetadata(path, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msc)\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m metadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparamMap\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m metadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparamMap\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mJavaMLReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mType\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mJavaMLReadable[PipelineModel]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcls\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    286\u001b[0m     uid, stages \u001b[38;5;241m=\u001b[39m PipelineSharedReadWrite\u001b[38;5;241m.\u001b[39mload(metadata, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msc, path)\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/ml/util.py:318\u001b[0m, in \u001b[0;36mJavaMLReader.load\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath should be a string, got type \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(path))\n\u001b[0;32m--> 318\u001b[0m java_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clazz, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_java\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis Java ML type cannot be loaded into Python currently: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clazz\n\u001b[1;32m    322\u001b[0m     )\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66m# Hide where the exception came from that shows a non-Pythonic\u001b[39m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66m# JVM exception message.\u001b[39m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
            "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: Error loading metadata: Expected class name org.apache.spark.ml.PipelineModel but found class name org.apache.spark.ml.classification.NaiveBayesModel"
          ]
        }
      ],
      "source": [
        "test_set=te\n",
        "loaded_model = NaiveBayesModel.load('pp')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4gRTQyBBBya"
      },
      "outputs": [],
      "source": [
        "n = 17  # Assuming n is defined somewhere\n",
        "hashingTF = HashingTF(inputCol=\"description_filtered\", outputCol=\"rawFeatures\", numFeatures=2**n)\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "\n",
        "# Apply HashingTF to the test set\n",
        "test_set_with_raw_features = hashingTF.transform(test_set)\n",
        "\n",
        "# Fit IDF on the training set to get the IDF model\n",
        "idf_model = idf.fit(test_set_with_raw_features)\n",
        "\n",
        "# Apply IDF to the test set\n",
        "test_set_with_features = idf_model.transform(test_set_with_raw_features)\n",
        "\n",
        "# Show the test set with the new features column\n",
        "test_set_with_features.select(\"description_filtered\", \"rawFeatures\", \"features\").show(5, truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iW5dii5QBBya",
        "outputId": "644b35bf-1479-4445-bbbf-2947ae26a6c9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r",
            "[Stage 238:>                                                        (0 + 1) / 1]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded model parameters:\n",
            "  featuresCol: features\n",
            "  labelCol: category_label\n",
            "  modelType: multinomial\n",
            "  predictionCol: prediction\n",
            "  probabilityCol: probability\n",
            "  rawPredictionCol: rawPrediction\n",
            "  smoothing: 1.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "loaded_model=NaiveBayesModel.load('pp')\n",
        "# Verify the model is loaded by checking its parameters or making predictions\n",
        "print(\"Loaded model parameters:\")\n",
        "for param, value in loaded_model.extractParamMap().items():\n",
        "    print(f\"  {param.name}: {value}\")\n",
        "\n",
        "# Making predictions on a new dataset\n",
        "# Assuming test_set is your test DataFrame\n",
        "predictions = loaded_model.transform(test_set_with_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8irEBr1BBya",
        "outputId": "76231dad-0286-4e1a-aee9-64e3ed0fa39e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/06/04 20:45:14 WARN DAGScheduler: Broadcasting large task binary with size 34.1 MiB\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+----------+\n",
            "|description_filtered|prediction|\n",
            "+--------------------+----------+\n",
            "|[apple, watch, ev...|      26.0|\n",
            "|[arthveda, fund, ...|       0.0|\n",
            "|[bank, india, hea...|       0.0|\n",
            "|[bank, looking, a...|       0.0|\n",
            "|[bank, may, accel...|       0.0|\n",
            "|[better, liquidit...|       0.0|\n",
            "|[bharti, axa, lif...|       0.0|\n",
            "|[bill, seek, repl...|       0.0|\n",
            "|[central, bank, a...|       0.0|\n",
            "|[cleanup, exercis...|       0.0|\n",
            "|[court, toss, 21,...|      27.0|\n",
            "|[credit, growth, ...|       0.0|\n",
            "|[credit, rating, ...|       0.0|\n",
            "|[deal, ergos, sta...|       0.0|\n",
            "|[describes, brexi...|       2.0|\n",
            "|[draftkings, repo...|       0.0|\n",
            "|[farmer, longer, ...|       0.0|\n",
            "|[financial, intel...|       0.0|\n",
            "|[firstquarter, ad...|       0.0|\n",
            "|[given, poor, fis...|       0.0|\n",
            "+--------------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "predictions.select('description_filtered','prediction').show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSyD5awwBByb"
      },
      "outputs": [],
      "source": [
        "#df.unpersist()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}