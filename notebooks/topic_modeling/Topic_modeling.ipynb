{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-qFVy2lBaqu"
      },
      "source": [
        "# Ropic modeling\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "5X0hK38jln02",
        "outputId": "3a43c669-15d1-4cf4-c44c-eb6dde0838cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=982bf4fa037f9d8515620684ab443ef327d1d7cdb7b285f940cbe5469e831b84\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## I- Modules import"
      ],
      "metadata": {
        "id": "mNfMRWDbFXDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.ml.feature import  IDF, HashingTF,CountVectorizer\n",
        "from pyspark.ml import  Pipeline\n",
        "from math import ceil,log2\n",
        "from pyspark.ml.classification import LogisticRegression,NaiveBayes,LogisticRegressionModel\n",
        "from pyspark.sql.functions import col,explode,split\n",
        "\n",
        "import numpy as np\n",
        "from pyspark.ml.clustering import LDA\n",
        "\n",
        "# Import Spark NLP\n",
        "#from sparknlp.base import *\n",
        "#from sparknlp.annotator import *\n",
        "#from sparknlp.pretrained import PretrainedPipeline\n",
        "#import sparknlp\n",
        "#from pyspark.ml.feature import CountVectorizer\n"
      ],
      "metadata": {
        "id": "3QYNQcr-FQLs"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## II- Spark context and session creation"
      ],
      "metadata": {
        "id": "Nj7Qu-EvK9_Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "scrolled": false,
        "id": "aFZWn6-QBByS",
        "outputId": "cc18968f-50af-4a70-9d51-126be08a96c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fe44acdab00>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://c67b9e22ab1e:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>TopicModeling</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "spark = (SparkSession.builder\n",
        "    #.master(\"spark://node02:7077\")\n",
        "    .appName(\"TopicModeling\")\n",
        "    #.config('spark.driver.cores','4')\n",
        "    #.config(\"spark.executorEnv.LD_LIBRARY_PATH\", \"/home/team1337/.local/easybuild_new/software/Python/3.8.6-GCCcore-10.2.0/lib:/home/team1337/.local/easybuild_new/software/libffi/3.3-GCCcore-10.2.0/lib64:/home/team1337/.local/easybuild_new/software/GMP/6.2.0-GCCcore-10.2.0/lib:/home/team1337/.local/easybuild_new/software/XZ/5.2.5-GCCcore-10.2.0/lib:/home/team1337/.local/easybuild_new/software/SQLite/3.33.0-GCCcore-10.2.0/lib:/home/team1337/.local/easybuild_new/software/Tcl/8.6.10-GCCcore-10.2.0/lib:/home/team1337/.local/easybuild_new/software/libreadline/8.0-GCCcore-10.2.0/lib:/home/team1337/.local/easybuild_new/software/ncurses/6.2-GCCcore-10.2.0/lib:/home/team1337/.local/easybuild_new/software/bzip2/1.0.8-GCCcore-10.2.0/lib:/home/team1337/.local/easybuild_new/software/binutils/2.35-GCCcore-10.2.0/lib:/home/team1337/.local/easybuild_new/software/zlib/1.2.11-GCCcore-10.2.0/lib:/home/team1337/.local/easybuild_new/software/GCCcore/10.2.0/lib64\") \\\n",
        "    #.config(\"spark.pyspark.python\", \"/home/team1337/.local/easybuild_new/software/Python/3.8.6-GCCcore-10.2.0/bin/python3\") \\\n",
        "    .getOrCreate()\n",
        "        )\n",
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## III- Dataframe preparing"
      ],
      "metadata": {
        "id": "PuV9CWoMN60A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Load the data"
      ],
      "metadata": {
        "id": "Em4ZWvqhPoHt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "xMI_45McvZto"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "df = spark.read.parquet(\"input/news.parquet\", header=True, inferSchema=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Partition and cache the dataframe"
      ],
      "metadata": {
        "id": "x4csdl2FOVf8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "4EXrJ3GIBByT",
        "outputId": "0d866061-6fee-4bbb-fd79-84c946204d72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "df.rdd.getNumPartitions()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "PPBefKKqBByU"
      },
      "outputs": [],
      "source": [
        "num_partitions=5*2\n",
        "df= df.repartition(num_partitions).cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ytFY1tjRBByU",
        "outputId": "cf5d7480-394e-43ac-e8e0-bc5cef64d4e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "df.rdd.getNumPartitions()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Preview the data"
      ],
      "metadata": {
        "id": "4G5XozIiQTEx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r86OcjwLv0EU",
        "outputId": "63e295fa-1b60-414b-f44f-3f8c8fd24a4b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1716608"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "sfy_ivN4BByV",
        "outputId": "c374612e-8073-411f-a842-736cbe7eef50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+--------------------+\n",
            "|category_label|description_filtered|\n",
            "+--------------+--------------------+\n",
            "|          11.0|thirst game inspi...|\n",
            "|          11.0|giant time herman...|\n",
            "|           7.0|iphone 12 series ...|\n",
            "|           8.0|8 new covid19 sli...|\n",
            "|           2.0|country largest t...|\n",
            "|          11.0|cambodia heartbre...|\n",
            "|          15.0|foxiness day crea...|\n",
            "|          11.0|world best summer...|\n",
            "|           4.0|elon musk claim p...|\n",
            "|          11.0|vine bridge nihon...|\n",
            "|          10.0|raf herbert simon...|\n",
            "|           9.0|5 steer setting b...|\n",
            "|           6.0|weird food allerg...|\n",
            "|          13.0|indigo girl exist...|\n",
            "|           1.0|instagram launch ...|\n",
            "|           8.0|provisional mansl...|\n",
            "|          11.0|amsterdam evolvin...|\n",
            "|           3.0|cost democracy lo...|\n",
            "|           3.0|white house take ...|\n",
            "|           3.0|donjon cuba surfa...|\n",
            "+--------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "id": "awWri7zZQcOl",
        "outputId": "80402c12-f32a-483f-9ed0-ddad43eda4c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- category_label: double (nullable = true)\n",
            " |-- description_filtered: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Convert filtered descriptions to arrays"
      ],
      "metadata": {
        "id": "VMpXsNNsO_Z6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Drqnhee0i26r",
        "outputId": "63a0cd7d-df8f-45c9-a6b5-ef7cf9bf7ebc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+------------------------------------------------------------------------------------------------------+\n",
            "|category_label|description_filtered                                                                                  |\n",
            "+--------------+------------------------------------------------------------------------------------------------------+\n",
            "|11.0          |[thirst, game, inspired, hotel, lionise, new, film]                                                   |\n",
            "|11.0          |[giant, time, hermanus, due, south, africa]                                                           |\n",
            "|7.0           |[iphone, 12, series, reportedly, support, beidou, navigation, news]                                   |\n",
            "|8.0           |[8, new, covid19, slip, elgin, oxford, 3, middlesexlondon, mon]                                       |\n",
            "|2.0           |[country, largest, tree, kauri, threatened, dieback, climate, change, hope, revered, specie]          |\n",
            "|11.0          |[cambodia, heartbreaking]                                                                             |\n",
            "|15.0          |[foxiness, day, create, play, funky, elephant, planter]                                               |\n",
            "|11.0          |[world, best, summer, address]                                                                        |\n",
            "|4.0           |[elon, musk, claim, pyramid, build, alien, get, criticised, invite, egypt]                            |\n",
            "|11.0          |[vine, bridge, nihon, testament, totally, reinvent, intend, bridge]                                   |\n",
            "|10.0          |[raf, herbert, simon, final, jil, electric, sander, appearance, brings, tear, support, ovation, photo]|\n",
            "|9.0           |[5, steer, setting, bound, kid, cyber, demeanor]                                                      |\n",
            "|6.0           |[weird, food, allergy, vino, sesame, seed]                                                            |\n",
            "|13.0          |[indigo, girl, existence, labeled, lesbian, duo, often, implies, mediocrity]                          |\n",
            "|1.0           |[instagram, launch, tiktok, copycat, calling, reel]                                                   |\n",
            "|8.0           |[provisional, manslayer, appointed, irish, capital, city, centre, breast, feeding, home]              |\n",
            "|11.0          |[amsterdam, evolving, kinship, weed]                                                                  |\n",
            "|3.0           |[cost, democracy, look, like]                                                                         |\n",
            "|3.0           |[white, house, take, muslim, travel, ban, supreme, court]                                             |\n",
            "|3.0           |[donjon, cuba, surface, stopping, point, feather, marco, rubio]                                       |\n",
            "+--------------+------------------------------------------------------------------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Create a new DataFrame with description_filtered as arrays\n",
        "df= df.withColumn('description_filtered', split(col('description_filtered'), ' '))\n",
        "# Show the new DataFrame\n",
        "df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IV- Feature Engineering\n"
      ],
      "metadata": {
        "id": "3RmBazOpQqXF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Explode the filtered descriptions to get the words"
      ],
      "metadata": {
        "id": "LjsTG0YLSyEn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "4qON2vCKBByX",
        "outputId": "3b8af783-1259-46dd-9346-ee5ee60033c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "|       col|\n",
            "+----------+\n",
            "|    thirst|\n",
            "|      game|\n",
            "|  inspired|\n",
            "|     hotel|\n",
            "|   lionise|\n",
            "|       new|\n",
            "|      film|\n",
            "|     giant|\n",
            "|      time|\n",
            "|  hermanus|\n",
            "|       due|\n",
            "|     south|\n",
            "|    africa|\n",
            "|    iphone|\n",
            "|        12|\n",
            "|    series|\n",
            "|reportedly|\n",
            "|   support|\n",
            "|    beidou|\n",
            "|navigation|\n",
            "+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "exploded_df=df.select(explode(df.description_filtered)).alias('words')\n",
        "exploded_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "DH41jOsrBByX"
      },
      "outputs": [],
      "source": [
        "#df=df.unpersist()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Get unique words in the filtered_description"
      ],
      "metadata": {
        "id": "MohTRQirTXD3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "2NbJFyQlBByX"
      },
      "outputs": [],
      "source": [
        "unique_words=exploded_df.distinct()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Cache and show the unique words dataframe"
      ],
      "metadata": {
        "id": "t-UjxthKU0Rq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "h1S8h39xBByX",
        "outputId": "8c884387-1347-4441-d313-56027086df18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+\n",
            "|        col|\n",
            "+-----------+\n",
            "|       hope|\n",
            "|     travel|\n",
            "|  traveling|\n",
            "|      still|\n",
            "|     outfit|\n",
            "|     laxity|\n",
            "|        art|\n",
            "|requirement|\n",
            "|      oscar|\n",
            "|      mammy|\n",
            "|       pant|\n",
            "| indigenous|\n",
            "|    melodic|\n",
            "| rejuvenate|\n",
            "|     online|\n",
            "|     lamott|\n",
            "|  connected|\n",
            "|   cautious|\n",
            "|      crest|\n",
            "|     monish|\n",
            "+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "unique_words=unique_words.cache()\n",
        "unique_words.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Get the vocabulary size"
      ],
      "metadata": {
        "id": "KCb1pLl3VLOL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "dCakXZ3PBByX",
        "outputId": "e54847cd-6dce-4cf7-ffcb-7770061fcf18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "128622"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "vocabulary_size=unique_words.count()\n",
        "vocabulary_size"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Unpersit the unique words dataframe(not needed anymore)"
      ],
      "metadata": {
        "id": "ycUp4mQDVbZ1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "nS-8eBNnBByX"
      },
      "outputs": [],
      "source": [
        "unique_words=unique_words.unpersist()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Get the smallest `n` such that $2^n$ is greater than `vocabulary_size`"
      ],
      "metadata": {
        "id": "irYncNP3WWhr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#num_features=log2(vocabulary_size)\n"
      ],
      "metadata": {
        "id": "9kdafIt4WnP8"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "H0nzNFsoBByY",
        "outputId": "9bfd5c77-c34f-44da-efb4-205229f02b2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "n=ceil(log2(vocabulary_size))\n",
        "n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Get the number of features for HashingTF"
      ],
      "metadata": {
        "id": "mtwfZn61Yzh-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_features=2**n\n",
        "num_features"
      ],
      "metadata": {
        "id": "FviyZsE_YxxT",
        "outputId": "d2a86ec1-65aa-430f-a263-82d33fccbe3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "131072"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Define the HashingTF and IDF stages"
      ],
      "metadata": {
        "id": "cV9aikKbYQzj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the HashingTF and IDF stages\n",
        "#hashingTF = HashingTF(inputCol=\"description_filtered\", outputCol=\"rawFeatures\", numFeatures=num_features)\n",
        "#idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")"
      ],
      "metadata": {
        "id": "bIL-PyIHYm4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv = CountVectorizer(inputCol=\"description_filtered\", outputCol=\"features\", vocabSize=80, minDF=3.0)\n",
        "# train the model\n",
        "cv_model = cv.fit(df)\n",
        "# transform the data. Output column name will be features.\n",
        "vectorized_tokens = cv_model.transform(df.limit(100))\n",
        "vectorized_tokens.show(truncate=False)"
      ],
      "metadata": {
        "id": "qYJp-K7O8pWN",
        "outputId": "dbdeed43-bf9e-40c4-ee66-ecd519909310",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+------------------------------------------------------------------------------------------------------+-----------------------------+\n",
            "|category_label|description_filtered                                                                                  |features                     |\n",
            "+--------------+------------------------------------------------------------------------------------------------------+-----------------------------+\n",
            "|11.0          |[thirst, game, inspired, hotel, lionise, new, film]                                                   |(80,[0],[1.0])               |\n",
            "|11.0          |[giant, time, hermanus, due, south, africa]                                                           |(80,[17],[1.0])              |\n",
            "|7.0           |[iphone, 12, series, reportedly, support, beidou, navigation, news]                                   |(80,[53],[1.0])              |\n",
            "|8.0           |[8, new, covid19, slip, elgin, oxford, 3, middlesexlondon, mon]                                       |(80,[0,16,54],[1.0,1.0,1.0]) |\n",
            "|2.0           |[country, largest, tree, kauri, threatened, dieback, climate, change, hope, revered, specie]          |(80,[],[])                   |\n",
            "|11.0          |[cambodia, heartbreaking]                                                                             |(80,[],[])                   |\n",
            "|15.0          |[foxiness, day, create, play, funky, elephant, planter]                                               |(80,[4],[1.0])               |\n",
            "|11.0          |[world, best, summer, address]                                                                        |(80,[19,31],[1.0,1.0])       |\n",
            "|4.0           |[elon, musk, claim, pyramid, build, alien, get, criticised, invite, egypt]                            |(80,[8],[1.0])               |\n",
            "|11.0          |[vine, bridge, nihon, testament, totally, reinvent, intend, bridge]                                   |(80,[],[])                   |\n",
            "|10.0          |[raf, herbert, simon, final, jil, electric, sander, appearance, brings, tear, support, ovation, photo]|(80,[1],[1.0])               |\n",
            "|9.0           |[5, steer, setting, bound, kid, cyber, demeanor]                                                      |(80,[14,60],[1.0,1.0])       |\n",
            "|6.0           |[weird, food, allergy, vino, sesame, seed]                                                            |(80,[76],[1.0])              |\n",
            "|13.0          |[indigo, girl, existence, labeled, lesbian, duo, often, implies, mediocrity]                          |(80,[],[])                   |\n",
            "|1.0           |[instagram, launch, tiktok, copycat, calling, reel]                                                   |(80,[],[])                   |\n",
            "|8.0           |[provisional, manslayer, appointed, irish, capital, city, centre, breast, feeding, home]              |(80,[33],[1.0])              |\n",
            "|11.0          |[amsterdam, evolving, kinship, weed]                                                                  |(80,[],[])                   |\n",
            "|3.0           |[cost, democracy, look, like]                                                                         |(80,[24,77],[1.0,1.0])       |\n",
            "|3.0           |[white, house, take, muslim, travel, ban, supreme, court]                                             |(80,[28,29,72],[1.0,1.0,1.0])|\n",
            "|3.0           |[donjon, cuba, surface, stopping, point, feather, marco, rubio]                                       |(80,[],[])                   |\n",
            "+--------------+------------------------------------------------------------------------------------------------------+-----------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## V- Models set up, training and evaluation"
      ],
      "metadata": {
        "id": "74T1zTMRxKP8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Set up Naive and Logistic regression classifiers"
      ],
      "metadata": {
        "id": "HmY4gqj5Q2w_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the classifiers\n",
        "\n",
        "# Logistic regression classifier\n",
        "lr = LogisticRegression(labelCol=\"category_label\", featuresCol=\"features\")\n",
        "\n",
        "# Naive Bayes classifier\n",
        "nb = NaiveBayes(labelCol=\"category_label\", featuresCol=\"features\")"
      ],
      "metadata": {
        "id": "f8ya3gJ_PxLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Set up pipelines\n",
        "\n",
        "We will  set up and returns  the pipelines of the following transformations for Native Bayes and Linear reggression\n",
        "\n",
        "- HashingTF\n",
        "- IDF\n",
        "- 3-Fold Cross-validation  without grid search"
      ],
      "metadata": {
        "id": "XT81OlVr5Ck1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LArI4unEDer1",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Define parameter grids\n",
        "paramGrid_nb = (ParamGridBuilder()\n",
        "        .addGrid(nb.smoothing, [0.5, 1.0, 2.0])\n",
        "        .build())\n",
        "\n",
        "paramGrid_nb=paramGrid_lr=ParamGridBuilder().build()\n",
        "\n",
        "# Create cross validators\n",
        "\n",
        "# Cross-validation for Naive Bayes\n",
        "cv_nb = CrossValidator(estimator=nb, estimatorParamMaps=paramGrid_nb,\n",
        "                        evaluator=MulticlassClassificationEvaluator(labelCol=\"category_label\", predictionCol=\"prediction\", metricName=\"accuracy\"),\n",
        "                        numFolds=3, parallelism=1)\n",
        "# Cross-validation for Logistic Regression\n",
        "cv_lr = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid_lr,\n",
        "                        evaluator=MulticlassClassificationEvaluator(labelCol=\"category_label\", predictionCol=\"prediction\", metricName=\"accuracy\"),\n",
        "                        numFolds=3, parallelism=1)\n",
        "\n",
        "\n",
        "# Create pipelines\n",
        "# Pipeline for Naive Bayes\n",
        "pipeline_nb = Pipeline(stages=[hashingTF, idf, cv_nb])\n",
        "# Pipeline for Logistic Regression\n",
        "pipeline_lr = Pipeline(stages=[hashingTF, idf, cv_lr])\n",
        "model_pipelines=pipeline_nb, pipeline_lr\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Split the data\n",
        "\n",
        "First of all let us split the data into train and test set: 80% for train and 20% for test"
      ],
      "metadata": {
        "id": "UD0gwMNf7yY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data\n",
        "(train_set, test_set) = df.randomSplit([0.8, 0.2], seed=0)"
      ],
      "metadata": {
        "id": "pThnAKRy8LqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Create a function for model training\n",
        "\n",
        "Let us create a function which takes as argument a model that it trains and then returns the trained model."
      ],
      "metadata": {
        "id": "M84xeE8i67YF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_topics = 20\n",
        "lda = LDA(k=num_topics, maxIter=10)\n",
        "model = lda.fit(vectorized_tokens)\n",
        "ll = model.logLikelihood(vectorized_tokens)\n",
        "lp = model.logPerplexity(vectorized_tokens)\n",
        "print(\"The lower bound on the log likelihood of the entire corpus: \" + str(ll))\n",
        "print(\"The upper bound on perplexity: \" + str(lp))"
      ],
      "metadata": {
        "id": "AvRF-E1-7ncY",
        "outputId": "a361d112-8d8c-43c1-c926-d4a562e61afd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The lower bound on the log likelihood of the entire corpus: -3057.1994367408975\n",
            "The upper bound on perplexity: 26.129909715734165\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extract vocabulary from CountVectorizer\n",
        "vocab = cv_model.vocabulary\n",
        "topics = model.describeTopics()\n",
        "topics_rdd = topics.rdd\n",
        "topics_words = topics_rdd\\\n",
        "       .map(lambda row: row['termIndices'])\\\n",
        "       .map(lambda idx_list: [vocab[idx] for idx in idx_list])\\\n",
        "       .collect()\n",
        "for idx, topic in enumerate(topics_words):\n",
        "    print(\"topic: {}\".format(idx))\n",
        "    print(\"*\"*25)\n",
        "    for word in topic:\n",
        "       print(word)\n",
        "    print(\"*\"*25)"
      ],
      "metadata": {
        "id": "Sab9GsSNpbf1",
        "outputId": "ea835406-2c19-4d73-e659-943b8355a5a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "topic: 0\n",
            "*************************\n",
            "kid\n",
            "state\n",
            "love\n",
            "picture\n",
            "news\n",
            "wa\n",
            "marriage\n",
            "week\n",
            "video\n",
            "police\n",
            "*************************\n",
            "topic: 1\n",
            "*************************\n",
            "republic\n",
            "video\n",
            "black\n",
            "bank\n",
            "marriage\n",
            "help\n",
            "family\n",
            "week\n",
            "wa\n",
            "coronavirus\n",
            "*************************\n",
            "topic: 2\n",
            "*************************\n",
            "ha\n",
            "college\n",
            "5\n",
            "covid19\n",
            "united\n",
            "need\n",
            "thing\n",
            "state\n",
            "death\n",
            "news\n",
            "*************************\n",
            "topic: 3\n",
            "*************************\n",
            "take\n",
            "white\n",
            "house\n",
            "nt\n",
            "find\n",
            "donald\n",
            "call\n",
            "right\n",
            "party\n",
            "help\n",
            "*************************\n",
            "topic: 4\n",
            "*************************\n",
            "american\n",
            "get\n",
            "best\n",
            "person\n",
            "help\n",
            "may\n",
            "first\n",
            "video\n",
            "human\n",
            "number\n",
            "*************************\n",
            "topic: 5\n",
            "*************************\n",
            "look\n",
            "like\n",
            "get\n",
            "american\n",
            "human\n",
            "thing\n",
            "video\n",
            "right\n",
            "want\n",
            "year\n",
            "*************************\n",
            "topic: 6\n",
            "*************************\n",
            "7\n",
            "ha\n",
            "party\n",
            "top\n",
            "people\n",
            "america\n",
            "family\n",
            "nt\n",
            "kid\n",
            "donald\n",
            "*************************\n",
            "topic: 7\n",
            "*************************\n",
            "say\n",
            "wa\n",
            "look\n",
            "school\n",
            "health\n",
            "top\n",
            "report\n",
            "child\n",
            "world\n",
            "woman\n",
            "*************************\n",
            "topic: 8\n",
            "*************************\n",
            "people\n",
            "may\n",
            "marriage\n",
            "woman\n",
            "number\n",
            "republic\n",
            "back\n",
            "death\n",
            "5\n",
            "america\n",
            "*************************\n",
            "topic: 9\n",
            "*************************\n",
            "video\n",
            "wa\n",
            "united\n",
            "party\n",
            "family\n",
            "10\n",
            "american\n",
            "marriage\n",
            "human\n",
            "call\n",
            "*************************\n",
            "topic: 10\n",
            "*************************\n",
            "photo\n",
            "trump\n",
            "7\n",
            "look\n",
            "help\n",
            "live\n",
            "10\n",
            "divorce\n",
            "show\n",
            "right\n",
            "*************************\n",
            "topic: 11\n",
            "*************************\n",
            "kid\n",
            "5\n",
            "food\n",
            "may\n",
            "thing\n",
            "america\n",
            "make\n",
            "house\n",
            "video\n",
            "news\n",
            "*************************\n",
            "topic: 12\n",
            "*************************\n",
            "united\n",
            "school\n",
            "say\n",
            "look\n",
            "back\n",
            "7\n",
            "get\n",
            "america\n",
            "food\n",
            "day\n",
            "*************************\n",
            "topic: 13\n",
            "*************************\n",
            "trump\n",
            "one\n",
            "10\n",
            "find\n",
            "week\n",
            "first\n",
            "new\n",
            "home\n",
            "u\n",
            "live\n",
            "*************************\n",
            "topic: 14\n",
            "*************************\n",
            "star\n",
            "first\n",
            "united\n",
            "bank\n",
            "say\n",
            "want\n",
            "make\n",
            "right\n",
            "may\n",
            "love\n",
            "*************************\n",
            "topic: 15\n",
            "*************************\n",
            "man\n",
            "united\n",
            "first\n",
            "state\n",
            "right\n",
            "black\n",
            "home\n",
            "news\n",
            "like\n",
            "way\n",
            "*************************\n",
            "topic: 16\n",
            "*************************\n",
            "coronavirus\n",
            "world\n",
            "death\n",
            "best\n",
            "ha\n",
            "year\n",
            "time\n",
            "america\n",
            "party\n",
            "wedding\n",
            "*************************\n",
            "topic: 17\n",
            "*************************\n",
            "covid19\n",
            "show\n",
            "back\n",
            "bank\n",
            "way\n",
            "world\n",
            "school\n",
            "love\n",
            "parent\n",
            "10\n",
            "*************************\n",
            "topic: 18\n",
            "*************************\n",
            "life\n",
            "police\n",
            "year\n",
            "new\n",
            "kid\n",
            "go\n",
            "divorce\n",
            "wa\n",
            "wedding\n",
            "take\n",
            "*************************\n",
            "topic: 19\n",
            "*************************\n",
            "may\n",
            "call\n",
            "u\n",
            "bank\n",
            "help\n",
            "say\n",
            "man\n",
            "life\n",
            "week\n",
            "party\n",
            "*************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Define a function to evaluate the model\n",
        "\n",
        "The function takes as parameter a fitted model, evaluates the model on train and test split and then return the train and test performance. The accuracy is the metric used."
      ],
      "metadata": {
        "id": "K-fBEB1f9aRD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIKVzBw1BByY"
      },
      "outputs": [],
      "source": [
        "# Initialize the evaluator\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"category_label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "\n",
        "# Function to evaluate model and get best parameters\n",
        "def evaluate_model(fitted_model):\n",
        "\n",
        "    print('Making predictions on the training set')\n",
        "\n",
        "    train_predictions = fitted_model.transform(train_set)\n",
        "\n",
        "    print('Making predictions on the test set')\n",
        "    test_predictions = fitted_model.transform(test_set)\n",
        "\n",
        "    print('Evaluating the model on training set')\n",
        "    train_accuracy = evaluator.evaluate(train_predictions)\n",
        "\n",
        "    print('Evaluating the model on test set')\n",
        "    test_accuracy = evaluator.evaluate(test_predictions)\n",
        "    return train_accuracy, test_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to evaluate model and get best parameters\n",
        "def evaluate_model(model, model_name):\n",
        "    print('Training the model')\n",
        "\n",
        "    # Train the model using cross-validation\n",
        "    fitted_model = model.fit(train_set)\n",
        "\n",
        "    # Get the best model from cross-validation\n",
        "    best_model = fitted_model.stages[-1].bestModel\n",
        "\n",
        "    print('Making predictions on the training set')\n",
        "    # Make predictions on the training set\n",
        "    train_predictions = fitted_model.transform(train_set)\n",
        "\n",
        "    print('Making predictions on the test set')\n",
        "    # Make predictions on the test set\n",
        "    test_predictions = fitted_model.transform(test_set)\n",
        "\n",
        "    # Initialize the evaluator\n",
        "    evaluator = MulticlassClassificationEvaluator(labelCol=\"category_label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "\n",
        "    print('Evaluating the model on training set')\n",
        "    # Evaluate the model on the training set\n",
        "    train_accuracy = evaluator.evaluate(train_predictions)\n",
        "\n",
        "    print('Evaluating the model on test set')\n",
        "    # Evaluate the model on the test set\n",
        "    test_accuracy = evaluator.evaluate(test_predictions)\n",
        "\n",
        "    print(f\"{model_name} Train Accuracy: {train_accuracy}\")\n",
        "    print(f\"{model_name} Test Accuracy: {test_accuracy}\")\n",
        "\n",
        "    # Print the best parameters\n",
        "    print(f\"Best parameters for {model_name}:\")\n",
        "\n",
        "    for param, value in best_model.extractParamMap().items():\n",
        "        print(f\"  {param.name}: {value}\")\n",
        "\n",
        "    return train_accuracy, test_accuracy,best_model"
      ],
      "metadata": {
        "id": "eJHhpCCj-jCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Create a function which takes pipelines and train the models, evaluate them and then return the results"
      ],
      "metadata": {
        "id": "yG6dMBjzLXWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate_models(model_pipelines,model_names=[\"Naive Bayes\", \"Logistic Regression\"]):\n",
        "\n",
        "    # Initialize the results dictionary\n",
        "    results = {}\n",
        "\n",
        "    # Loop over the indices and model names simultaneously\n",
        "    for idx, (model_pipeline, model_name) in enumerate(zip(model_pipelines, model_names)):\n",
        "        print(f\"Training {model_name} model\")\n",
        "\n",
        "        # Fit the model pipeline to the training set\n",
        "        fitted_model = model_pipeline.fit(train_set)\n",
        "\n",
        "        print(\"Done\")\n",
        "        print(f\"Evaluating {model_name} model\")\n",
        "\n",
        "        # Evaluate the fitted model\n",
        "        train_accuracy, test_accuracy, best_model = evaluate_model(fitted_model)\n",
        "\n",
        "        # Store the results\n",
        "        results[idx] = {\n",
        "            'model_name': model_name,\n",
        "            #'pipeline': model_pipeline,\n",
        "            'fitted_model': fitted_model,\n",
        "            \"train_accuracy\": train_accuracy}\n",
        "\n",
        "        if len(model_name)==0:\n",
        "            results=results[0]\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "8MmY_Fp54MXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Call the function and interpret the results"
      ],
      "metadata": {
        "id": "pYDcrjUHBQHf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = train_and_evaluate_models()"
      ],
      "metadata": {
        "id": "9T4JAj-QId2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results[0]"
      ],
      "metadata": {
        "id": "ycLhdvlMJ8nB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results[1]"
      ],
      "metadata": {
        "id": "OQNk6X_RKBuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We remark that\n",
        "- Naive Bayes\n",
        "- Logistic regression\n",
        "\n",
        "We can then conclude that t\n",
        "- he two models set a good performance on both training and test set.\n",
        "- The Logistic regression models outperforms the Naive Bayes model\n",
        "\n",
        "In the next section, we will tune the parameters of the Naive bayes to get the best parameters."
      ],
      "metadata": {
        "id": "808-IeqCNb1v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VI- Logistic regression hyperparameters tuning"
      ],
      "metadata": {
        "id": "xkME6ByNOegG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Pipeline creation"
      ],
      "metadata": {
        "id": "pAWi0NqwSzgp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parameter grids for Logistic regresion grid search\n",
        "reg_values = np.logspace(-4, 4, num=100)\n",
        "l1_ratios = np.linspace(0, 1, num=10)\n",
        "\n",
        "paramGrid_lr=paramGrid_lr.addGrid(lr.regParam, reg_values).build()\n",
        "\n",
        "# Create Cross-validation for Logistic Regression\n",
        "cv_lr = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid_lr,\n",
        "                        evaluator=MulticlassClassificationEvaluator(labelCol=\"category_label\", predictionCol=\"prediction\", metricName=\"accuracy\"),\n",
        "                        numFolds=3, parallelism=1)\n",
        "\n",
        "\n",
        "# Create pipeline for Logistic Regression\n",
        "pipeline_lr = Pipeline(stages=[hashingTF, idf, cv_lr])\n",
        "\n",
        "pipeline_lr"
      ],
      "metadata": {
        "id": "Wtq5K0JbOdpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Hyperparameters tuning"
      ],
      "metadata": {
        "id": "0XhMJxEzS87h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results=train_and_evaluate_models(model_pipelines=[pipeline_lr],model_names=[\"Logistic Regression\"])\n",
        "results"
      ],
      "metadata": {
        "id": "4bSLe3K5TL1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Get the best parameters"
      ],
      "metadata": {
        "id": "yUlLvsWVTkDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fitted_model=results['fitted_model']\n",
        "\n",
        "# Get the best model\n",
        "best_model = fitted_model.stages[-1].bestModel\n",
        "\n",
        "# Print the best parameters\n",
        "print(f\"Best parameters for Logistic regression:\")\n",
        "\n",
        "for param, value in best_model.extractParamMap().items():\n",
        "     print(f\"  {param.name}: {value}\")"
      ],
      "metadata": {
        "id": "k5rCb_boTk3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Save the best model"
      ],
      "metadata": {
        "id": "uEH0OfIqUxOg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xX4pmvAVBByZ",
        "outputId": "843aa553-e419-43ba-dbb3-3ebb1a67f454"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/06/04 20:03:43 WARN TaskSetManager: Stage 216 contains a task of very large size (33450 KiB). The maximum recommended task size is 1000 KiB.\n",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "best_model.save('output/news_categorization_model')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VII- Summary\n",
        "\n",
        "In this notebook we have studied two models for our news categorization task. There are Naive Bayes and Logistic regression.\n",
        "\n",
        " Our study reveals that the Logistic regression was the one with best performance.\n",
        "\n",
        " Then we tunned the Logistic regression hyperparameters using grid search and then we find the best model that we save.\n",
        "\n",
        " The next step of our work will be to ..."
      ],
      "metadata": {
        "id": "GnFsw8x5Vyy2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSyD5awwBByb"
      },
      "outputs": [],
      "source": [
        "#df.unpersist()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}