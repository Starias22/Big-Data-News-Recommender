x-airflow-image: &airflow_image starias/airflow-spark
x-kafka-image: &kafka_image confluentinc/cp-kafka:latest

x-spark-common-properties: &spark_common_properties
  image: starias/spark-kafka
  volumes:
   - .jobs:/opt/bitnami/spark/jobs
  networks:
    - kafka
    - backend
  restart: always
  
x-spark-worker-environment: &spark_worker_environment
  SPARK_MODE: worker
  SPARK_WORKER_CORES: 2
  SPARK_WORKER_MEMORY: 8G
  SPARK_MASTER_URL: spark://spark-master:7077
  TRAINED_MODELS_PATH: ${TRAINED_MODELS_PATH}

x-environment: &airflow_environment
  - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
  - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://airflow:airflow@postgres/airflow
  - AIRFLOW__CELERY__BROKER_URL=redis://:@redis:6379/2
  - AIRFLOW__CORE__FERNET_KEY=''
  #- AIRFLOW__CORE__LOAD_DEFAULT_CONNECTIONS=False
  - AIRFLOW__DETABASE__LOAD_DEFAULT_CONNECTIONS=False
  - AIRFLOW__CORE__LOAD_EXAMPLES=False
  - AIRFLOW__CORE__STORE_DAG_CODE=True
  - AIRFLOW__CORE__STORE_SERIALIZED_DAGS=True
  - AIRFLOW__WEBSERVER__EXPOSE_CONFIG=True
  - AIRFLOW_CONN_POSTGRES_DEFAULT=postgresql://airflow:airflow@postgres/airflow
  - AIRFLOW__CORE__FERNET_KEY=EJ7tt4r06HhjniZKjNICTqaOGotA2Jfa9EF7KyW-7Wc=
  - KAFKA_BOOTSTRAP_SERVERS=${KAFKA_BOOTSTRAP_SERVERS}
  - AIRFLOW__CELERY__WORKERS=2
  - AIRFLOW__CELERY__WORKER_CONCURRENCY=2
  #- AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:B9#fS!4qW@p8zK*1@postgres/airflow-db
  - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
  - CHECKPOINT_DIR=${CHECKPOINT_DIR}
  - REDIS_HOST=${REDIS_HOST}
  - MONGO_DB_URI=${MONGO_DB_URI}
  - AIRFLOW__CORE__DEFAULT_TIMEZONE=UTC
  - AIRFLOW__EMAIL__EMAIL_BACKEND=airflow.utils.email.send_email_smtp
  - AIRFLOW__SMTP__SMTP_HOST=smtp.gmail.com
  - AIRFLOW__SMTP__START_TLS=False
  - AIRFLOW__SMTP__SMTP_SSL=False
  - AIRFLOW__SMTP__SMTP_PORT=587
  - TRAINED_MODELS_PATH=${TRAINED_MODELS_PATH}
  
services:
  spark-master:
    <<: *spark_common_properties
    container_name: spark-master
    restart: always
    ports:
      - "9090:8080"
      - "7077:7077"
    command: bin/spark-class org.apache.spark.deploy.master.Master
    volumes:
      - ./trained_models:/opt/trained_models/
      - ./data/checkpoint/filtered_news/:/opt/airflow/checkpoint/filtered_news/
      - ./data/checkpoint/processed_news/:/opt/airflow/checkpoint/processed_news/
      - ./data/checkpoint/available_news/:/opt/airflow/checkpoint/available_news/
    networks:
      - backend
      - kafka
    environment:
      - TRAINED_MODELS_PATH=${TRAINED_MODELS_PATH}
      #- SPARK_MODE=master
      #- SPARK_RPC_AUTHENTICATION_ENABLED=no
      #- SPARK_RPC_ENCRYPTION_ENABLED=no

  spark-worker1:
    <<: *spark_common_properties
    container_name: spark-worker1
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    depends_on:
      - spark-master
    environment: *spark_worker_environment
    volumes:
      - ./trained_models:/opt/trained_models
      - ./nltk_data:/opt/nltk_data/
      - ./data/checkpoint/filtered_news/:/opt/airflow/checkpoint/filtered_news/
      - ./data/checkpoint/processed_news/:/opt/airflow/checkpoint/processed_news/
      - ./data/checkpoint/available_news/:/opt/airflow/checkpoint/available_news/
  redis:
    image: redis:latest
    container_name: redis
    ports:
      - "6379:6379"
    networks:
      - backend
    volumes:
      - ./data/redis:/data
    restart: always
  
  mongodb:
    restart: always
    image: mongo:latest
    container_name: mongodb
    #environment:
      #- MONGO_INITDB_ROOT_USERNAME=admin
      #- MONGO_INITDB_ROOT_PASSWORD=password
    ports:
      - "27017:27017"
    volumes:
      - ./data/mongodb:/data/db  
    networks:
      - backend

  zookeeper:
    restart: always
    image: confluentinc/cp-zookeeper:latest
    container_name: zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    volumes:
      - ./data//zookeeper/data:/var/lib/zookeeper/data
      - ./data/zookeeper/log:/var/lib/zookeeper/log
    networks:
      - kafka

  kafka-broker1:
    image: *kafka_image
    depends_on:
      - zookeeper
    restart: always
    container_name: kafka-broker1
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-broker1:9092  # Use service name for internal Docker DNS
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
      KAFKA_LOG_DIRS: /broker
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'false'  # Disable auto topic creation
    volumes:
      - ./data/kafka/log/broker1:/broker
      - ./scripts/kafka-scripts/:/scripts
    networks:
      - kafka

  kafka-ui:
    container_name: kafka-ui
    image: provectuslabs/kafka-ui:latest
    volumes:
      - ./kafka-ui/config.yml:/etc/kafkaui/dynamic_config.yaml
    environment:
      DYNAMIC_CONFIG_ENABLED: 'true'
    depends_on:
      - kafka-broker1
    networks:
      - kafka
    ports:
      - '7070:8080'
    healthcheck:
      test: wget --no-verbose --tries=1 --spider localhost:8080 || exit 1
      interval: 5s
      timeout: 10s
      retries: 3
      start_period: 30s
  
  postgres:
    restart: always
    image: postgres:13
    container_name: postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    networks:
      - backend
    volumes:
      - ./data/postgres:/var/lib/postgresql/data

  airflow-flower:
        image: *airflow_image
        container_name: airflow-flower
        restart: always
        depends_on:
            - redis
        environment: *airflow_environment
        ports:
            - "5555:5555"
        command: celery flower
        networks:
          - backend

  airflow-init:
    container_name: airflow-init
    image: *airflow_image
    depends_on:
      - postgres
    environment: *airflow_environment
    secrets:
    - airflow_user
    - airflow_email
    - airflow_password
    - airflow_firstname
    - airflow_lastname
    volumes:
      - ./scripts/airflow-scripts/airflow-init-entrypoint.sh:/entrypoint.sh
    entrypoint: ["/bin/bash", "/entrypoint.sh"]
    networks:
      - backend
      - kafka
    

  airflow-webserver:
    restart: always
    container_name: airflow-webserver
    image: *airflow_image
    depends_on:
      - airflow-init
      - kafka-broker1
    environment: *airflow_environment
    ports:
      - "8080:8080"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./trained_models:/opt/trained_models/
      - ./data/airflow-logs:/opt/airflow/logs
      - ./config:/opt/config
      - ./scripts/airflow-scripts/entrypoint.sh:/entrypoint.sh
    command: webserver
    secrets:
      - smtp_user
      - smtp_password
      - smtp_mail_from
    entrypoint: ["/bin/bash", "/entrypoint.sh"]
    networks:
      - backend
      - kafka

  airflow-scheduler:
    restart: always
    container_name: airflow-scheduler 
    image: *airflow_image
    depends_on: 
      - airflow-init
    environment: *airflow_environment
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./trained_models:/opt/trained_models/
      - ./data/airflow-logs:/opt/airflow/logs
      - ./config:/opt/config
      - ./src:/opt/src
      - ./scripts/airflow-scripts/entrypoint.sh:/entrypoint.sh   
    command: scheduler
    networks:
      - backend
      - kafka
    secrets:
      - smtp_user
      - smtp_password
      - smtp_mail_from
    entrypoint: ["/bin/bash", "/entrypoint.sh"]
    
  airflow-worker:
    container_name: airflow-worker
    image: *airflow_image
    depends_on:
      - airflow-scheduler
    environment: *airflow_environment
    networks:
      - backend
      - kafka
    restart: always
    volumes:
      - ./airflow/dags:/opt/airflow/dags/
      - ./data/airflow-logs:/opt/airflow/logs
      - ./config/:/opt/config/
      - ./src/:/opt/src/
      - ./scripts:/opt/scripts/
      - ./trained_models/:/opt/trained_models/
      #- /opt/bitnami/spark/jars/:/opt/bitnami/spark/jars/
      - ./data/checkpoint/filtered_news/:/opt/airflow/checkpoint/filtered_news/
      - ./data/checkpoint/processed_news/:/opt/airflow/checkpoint/processed_news/
      - ./data/checkpoint/available_news/:/opt/airflow/checkpoint/available_news/
      - ./scripts/airflow-scripts/entrypoint.sh:/entrypoint.sh
    command: celery worker
    secrets:
      - smtp_user
      - smtp_password
      - smtp_mail_from
    entrypoint: ["/bin/bash", "/entrypoint.sh"]

  newsengine-client:
    restart: always
    image: starias/newsengine-client:latest
    container_name: newsengine-client
    depends_on:
      - kafka-broker1
      - mongodb
      - spark-master
    ports:
      - "5000:5000"
    volumes:
      - .:/app  # Mount the current directory to /app in the container
    environment:
      - PYTHON_ENV=development
      - MONGO_DB_URI=${MONGO_DB_URI}
      - KAFKA_BOOTSTRAP_SERVERS=${KAFKA_BOOTSTRAP_SERVERS}
      #- FLASK_APP=${FLASK_APP}
      #- FLASK_ENV=development # doesn apply debug mode as expected 
    networks:
      - backend
      - kafka
    entrypoint: >
      flask  --app=app/app.py run --host=0.0.0.0 --port=5000 --debug

networks:
  backend:
    name: backend-network 
    driver: bridge
  kafka:
    name: kafka-network  
    driver: bridge

secrets:
  smtp_user:
    file: ./secrets/smtp_user.txt
  smtp_password:
    file: ./secrets/smtp_password.txt
  smtp_mail_from:
    file: ./secrets/smtp_mail_from.txt
  airflow_user:
    file: ./secrets/airflow_user/username.txt
  airflow_email:
    file: ./secrets/airflow_user/email.txt
  airflow_password:
    file: ./secrets/airflow_user/password.txt
  airflow_firstname:
    file: ./secrets/airflow_user/firstname.txt
  airflow_lastname:
    file: ./secrets/airflow_user/lastname.txt

volumes:
  postgres_data:
    